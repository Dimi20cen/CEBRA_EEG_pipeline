{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-markdown",
   "metadata": {},
   "source": [
    "## EEG Data Classification: Neural State Prediction\n",
    "\n",
    "Dataset: 'EEG data - Sheet1.csv'\n",
    "\n",
    "Task: Classify participant data into two neural states.\n",
    "\n",
    "**Workflow Overview:**\n",
    "1. Load data.\n",
    "2. **Task 1: Model Training & Evaluation**\n",
    "   - Engineer features (regional averages, asymmetry).\n",
    "   - Select features based on prior EDA.\n",
    "   - Preprocess data (scaling, outlier removal, correlation filtering).\n",
    "   - Setup modeling parameters (CV, pipelines, grids).\n",
    "   - Run baseline model.\n",
    "   - Train and evaluate models (Logistic Regression, SVM, kNN) using GridSearchCV.\n",
    "   - Report Accuracy and Precision.\n",
    "3. **Task 2: Feature Importance Analysis**\n",
    "   - Part A: Analyze original 320 features (UFS, RFE, PCA).\n",
    "   - Part B: Analyze features used in Task 1 models (UFS, RFE, PCA).\n",
    "4. Summarize results and provide reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Scikit-learn imports for modeling and evaluation\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV, cross_validate\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, make_scorer, confusion_matrix\n",
    "\n",
    "# Scikit-learn imports for feature selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-markdown",
   "metadata": {},
   "source": [
    "## Configuration and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration Flags and Thresholds ---\n",
    "REMOVE_OUTLIER_SAMPLE = True # Flag to remove Sample #17, identified as an outlier\n",
    "OUTLIER_SAMPLE_INDEX = 16    # 0-based index for Sample #17\n",
    "CORRELATION_THRESHOLD = 0.93 # Threshold for removing highly correlated features\n",
    "N_SPLITS_CV = 5              # Number of splits for K-Fold CV\n",
    "N_REPEATS_CV = 5             # Number of repeats for K-Fold CV\n",
    "K_TOP_FEATURES = 5           # Number of top features to identify in Task 2\n",
    "RANDOM_STATE = 42            # Seed for reproducibility\n",
    "DATA_FILEPATH = 'EEG data - Sheet1.csv' # Path to the dataset\n",
    "\n",
    "# --- EEG Channel Mapping (EGI 64 to 10-10 System) ---\n",
    "EEG_MAPPING_TEXT = \"\"\"\n",
    "1 → 1 → F10\n",
    "2 → 3 → AF4\n",
    "3 → 4 → F2\n",
    "4 → 6 → Fcz\n",
    "5 → 9 → FP2\n",
    "6 → 11 → Fz\n",
    "7 → 13 → FC1\n",
    "8 → 16 → AFz\n",
    "9 → 19 → F1\n",
    "10 → 22 → FP1\n",
    "11 → 23 → AF3\n",
    "12 → 24 → F3\n",
    "13 → 27 → F5\n",
    "14 → 28 → FC5\n",
    "15 → 29 → FC3\n",
    "16 → 30 → C1\n",
    "17 → 32 → F9\n",
    "18 → 33 → F7\n",
    "19 → 34 → FT7\n",
    "20 → 36 → C3\n",
    "21 → 37 → CP1\n",
    "22 → 41 → C5\n",
    "23 → 44 → T9\n",
    "24 → 45 → T7\n",
    "25 → 46 → Tp7\n",
    "26 → 47 → Cp5\n",
    "27 → 51 → P5\n",
    "28 → 52 → P3\n",
    "29 → 57 → TP9\n",
    "30 → 58 → P7\n",
    "31 → 60 → P1\n",
    "32 → 64 → P9\n",
    "33 → 67 → PO3\n",
    "34 → 62 → Pz\n",
    "35 → 70 → O1\n",
    "36 → 72 → Poz\n",
    "37 → 75 → Oz\n",
    "38 → 77 → PO4\n",
    "39 → 83 → O2\n",
    "40 → 85 → P2\n",
    "41 → 87 → Cp2\n",
    "42 → 92 → P4\n",
    "43 → 95 → P10\n",
    "44 → 96 → P8\n",
    "45 → 97 → P6\n",
    "46 → 98 → CP6\n",
    "47 → 100 → TP10\n",
    "48 → 102 → TP8\n",
    "49 → 103 → C6\n",
    "50 → 104 → C4\n",
    "51 → 105 → C2\n",
    "52 → 108 → T8\n",
    "53 → 111 → FC4\n",
    "54 → 112 → FC2\n",
    "55 → 114 → T10\n",
    "56 → 117 → FC6\n",
    "57 → 118 → N/A\n",
    "58 → 122 → F8\n",
    "59 → 123 → F6\n",
    "60 → 124 → F4\n",
    "61 → 125 → N/A\n",
    "62 → 126 → N/A\n",
    "63 → 127 → N/A\n",
    "64 → 128 → N/A\n",
    "\"\"\"\n",
    "\n",
    "# --- Feature Definitions (Based on EDA) ---\n",
    "TOP_FEATURE_DEFINITIONS = {\n",
    "    \"delta50\": 50, \"delta42\": 42, \"delta24\": 24, \"delta12\": 12, \"theta50\": 50,\n",
    "    \"delta52\": 52, \"theta42\": 42, \"theta24\": 24, \"theta52\": 52, \"theta12\": 12,\n",
    "}\n",
    "TOP_REGIONAL_FEATURES_DEF = [\n",
    "    'central_right_delta_mean', 'parietal_right_delta_mean', 'temporal_left_delta_mean',\n",
    "    'frontal_left_delta_mean', 'central_right_theta_mean', 'temporal_right_delta_mean',\n",
    "    'parietal_right_theta_mean', 'temporal_left_theta_mean', 'temporal_right_theta_mean'\n",
    "]\n",
    "TOP_ASYMMETRY_FEATURES_DEF = [\n",
    "    'frontal_left_frontal_right_delta_asymmetry', 'central_left_central_right_delta_asymmetry',\n",
    "    'temporal_left_temporal_right_delta_asymmetry', 'parietal_left_parietal_right_delta_asymmetry',\n",
    "    'frontal_left_frontal_right_theta_asymmetry', 'central_left_central_right_theta_asymmetry'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-helpers-markdown",
   "metadata": {},
   "source": [
    "## General Helper Functions\n",
    "\n",
    "These functions are used across different steps of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "general-helpers-code",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_eeg_mapping(mapping_text):\n",
    "    \"\"\"Parses the multiline EGI 64 to 10-10 mapping string into a dictionary.\"\"\"\n",
    "    mapping = {}\n",
    "    lines = mapping_text.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        # Regex to capture EGI index (group 1) and 10-10 label (group 2)\n",
    "        match = re.match(r'(\\d+)\\s*→\\s*\\d+\\s*→\\s*(.*)', line)\n",
    "        if match:\n",
    "            egi_64_index = int(match.group(1))\n",
    "            label_10_10 = match.group(2).strip()\n",
    "            mapping[egi_64_index] = label_10_10\n",
    "    return mapping\n",
    "\n",
    "def compute_regional_averages(df, electrode_map):\n",
    "    \"\"\"Computes average band power for predefined neuroanatomical regions.\"\"\"\n",
    "    # Define regions based on 10-10 system labels\n",
    "    regions = {\n",
    "        'frontal_left': ['F1', 'F3', 'F5', 'F7', 'F9', 'AF3'],\n",
    "        'frontal_right': ['F2', 'F4', 'F6', 'F8', 'F10', 'AF4'],\n",
    "        'central_left': ['C1', 'C3', 'C5'],\n",
    "        'central_right': ['C2', 'C4', 'C6'],\n",
    "        'temporal_left': ['T7', 'T9', 'TP7', 'TP9'],\n",
    "        'temporal_right': ['T8', 'T10', 'TP8', 'TP10'],\n",
    "        'parietal_left': ['P1', 'P3', 'P5', 'P7', 'P9', 'PO3'],\n",
    "        'parietal_right': ['P2', 'P4', 'P6', 'P8', 'P10', 'PO4'],\n",
    "        'occipital_left': ['O1'],\n",
    "        'occipital_right': ['O2'],\n",
    "        'midline': ['Fz', 'FCz', 'Cz', 'CPz', 'Pz', 'POz', 'Oz']\n",
    "    }\n",
    "\n",
    "    # Create reverse mapping: 10-10 label -> EGI channel number\n",
    "    rev_map = {label: ch_num for ch_num, label in electrode_map.items() if label != 'N/A'}\n",
    "\n",
    "    regional_features = pd.DataFrame(index=df.index)\n",
    "    bands = ['alpha', 'beta', 'delta', 'theta', 'gamma']\n",
    "\n",
    "    # Calculate mean for each region and band\n",
    "    for region_name, electrode_labels in regions.items():\n",
    "        for band in bands:\n",
    "            # Get EGI channel numbers for the current region\n",
    "            channel_indices = [rev_map[label] for label in electrode_labels if label in rev_map]\n",
    "\n",
    "            if channel_indices:\n",
    "                # Construct column names (e.g., 'alpha1', 'alpha3', ...)\n",
    "                band_cols = [f\"{band}{idx}\" for idx in channel_indices if f\"{band}{idx}\" in df.columns]\n",
    "\n",
    "                if band_cols:\n",
    "                    # Compute mean across the relevant channel columns for each sample\n",
    "                    feature_name = f\"{region_name}_{band}_mean\"\n",
    "                    regional_features[feature_name] = df[band_cols].mean(axis=1)\n",
    "\n",
    "    print(f\"Generated {regional_features.shape[1]} regional average features.\")\n",
    "    return regional_features\n",
    "\n",
    "def compute_asymmetry_features(regional_features_df):\n",
    "    \"\"\"Calculates hemispheric asymmetry features based on regional averages.\"\"\"\n",
    "    asymmetry_df = pd.DataFrame(index=regional_features_df.index)\n",
    "    epsilon = 1e-10 # To avoid division by zero\n",
    "\n",
    "    # Define symmetric region pairs for asymmetry calculation\n",
    "    region_pairs = [\n",
    "        ('frontal_left', 'frontal_right'), ('central_left', 'central_right'),\n",
    "        ('temporal_left', 'temporal_right'), ('parietal_left', 'parietal_right'),\n",
    "        ('occipital_left', 'occipital_right')\n",
    "    ]\n",
    "    bands = ['alpha', 'beta', 'delta', 'theta', 'gamma']\n",
    "\n",
    "    for left_region, right_region in region_pairs:\n",
    "        for band in bands:\n",
    "            left_col = f\"{left_region}_{band}_mean\"\n",
    "            right_col = f\"{right_region}_{band}_mean\"\n",
    "\n",
    "            # Check if both regional features exist\n",
    "            if left_col in regional_features_df.columns and right_col in regional_features_df.columns:\n",
    "                left_values = regional_features_df[left_col]\n",
    "                right_values = regional_features_df[right_col]\n",
    "                feature_name = f\"{left_region}_{right_region}_{band}_asymmetry\"\n",
    "                # Calculate normalized difference\n",
    "                asymmetry_df[feature_name] = (left_values - right_values) / (left_values + right_values + epsilon)\n",
    "\n",
    "    print(f\"Generated {asymmetry_df.shape[1]} hemispheric asymmetry features.\")\n",
    "    return asymmetry_df\n",
    "\n",
    "def filter_correlated_features(X, feature_names, threshold=0.97):\n",
    "    \"\"\"Removes highly correlated features to reduce redundancy.\"\"\"\n",
    "    if X.shape[1] <= 1:\n",
    "        return X, feature_names\n",
    "\n",
    "    df_corr = pd.DataFrame(X, columns=feature_names)\n",
    "    corr_matrix = df_corr.corr().abs()\n",
    "\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Find features with correlation greater than the threshold\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "    if to_drop:\n",
    "        print(f\"Dropping {len(to_drop)} highly correlated features (threshold={threshold}): {to_drop}\")\n",
    "        # Get indices of features to keep\n",
    "        keep_indices = [i for i, name in enumerate(feature_names) if name not in to_drop]\n",
    "        # Filter X and feature_names\n",
    "        X_filtered = X[:, keep_indices]\n",
    "        filtered_names = [name for name in feature_names if name not in to_drop]\n",
    "        return X_filtered, filtered_names\n",
    "    else:\n",
    "        print(\"No highly correlated features found above the threshold.\")\n",
    "        return X, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-load-data-markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "step1-load-data-func-def",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_raw = pd.read_csv(DATA_FILEPATH, index_col=0)\n",
    "# print(f\"Dataset loaded: {df.shape[0]} samples, {df_raw.shape[1]} columns (including target)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "step1-load-data-execute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw = load_data(DATA_FILEPATH)\n",
    "# if df_raw is None:\n",
    "#     raise SystemExit(\"Stopping: Data loading error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-feature-eng-markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "step2-feature-eng-func-def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df, mapping_text):\n",
    "    \"\"\"Engineers regional and asymmetry features.\"\"\"\n",
    "    print(\"\\n--- Task 1: Feature Engineering --- \")\n",
    "    channel_map = parse_eeg_mapping(mapping_text)\n",
    "    regional_features = compute_regional_averages(df, channel_map)\n",
    "    asymmetry_features = compute_asymmetry_features(regional_features)\n",
    "    df_engineered = pd.concat([df, regional_features, asymmetry_features], axis=1)\n",
    "    print(f\"Shape after adding engineered features: {df_engineered.shape}\")\n",
    "    return df_engineered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "step2-feature-eng-execute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Task 1: Feature Engineering --- \n",
      "Generated 55 regional average features.\n",
      "Generated 25 hemispheric asymmetry features.\n",
      "Shape after adding engineered features: (40, 401)\n"
     ]
    }
   ],
   "source": [
    "df_engineered = engineer_features(df_raw, EEG_MAPPING_TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-feature-select-markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Feature Selection (EDA-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "step3-feature-select-func-def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_eda_features(df_engineered, top_defs, regional_defs, asymmetry_defs):\n",
    "    \"\"\"Selects features based on predefined lists from EDA.\"\"\"\n",
    "    print(\"\\n--- Task 1: Feature Selection (Based on prior EDA) ---\")\n",
    "    original_cols = [col for col in df_engineered.columns if not ('_mean' in col or '_asymmetry' in col or 'target' in col)]\n",
    "    top_individual_cols = []\n",
    "    for name, ch_num in top_defs.items():\n",
    "        band = re.match(r\"([a-zA-Z]+)\", name).group(1)\n",
    "        col_name = f\"{band}{ch_num}\"\n",
    "        if col_name in original_cols: top_individual_cols.append(col_name)\n",
    "        else: print(f\"Warning: Defined top feature '{col_name}' not found.\")\n",
    "    top_individual_cols = sorted(list(set(top_individual_cols)))\n",
    "    print(f\"Selected {len(top_individual_cols)} individual channel features.\")\n",
    "\n",
    "    top_regional_cols = [col for col in regional_defs if col in df_engineered.columns]\n",
    "    print(f\"Selected {len(top_regional_cols)} regional average features.\")\n",
    "\n",
    "    top_asymmetry_cols = [col for col in asymmetry_defs if col in df_engineered.columns]\n",
    "    print(f\"Selected {len(top_asymmetry_cols)} asymmetry features.\")\n",
    "\n",
    "    selected_feature_names_eda = top_individual_cols + top_regional_cols + top_asymmetry_cols\n",
    "    print(f\"Total features selected based on EDA definitions: {len(selected_feature_names_eda)}\")\n",
    "\n",
    "    cols_to_keep = ['target'] + selected_feature_names_eda\n",
    "    cols_to_keep = [col for col in cols_to_keep if col in df_engineered.columns]\n",
    "    df_selected = df_engineered[cols_to_keep].copy()\n",
    "    print(f\"Shape after selecting EDA-based features: {df_selected.shape}\")\n",
    "\n",
    "    final_selected_names = [name for name in selected_feature_names_eda if name in df_selected.columns]\n",
    "    return df_selected, final_selected_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "step3-feature-select-execute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Task 1: Feature Selection (Based on prior EDA) ---\n",
      "Selected 10 individual channel features.\n",
      "Selected 9 regional average features.\n",
      "Selected 6 asymmetry features.\n",
      "Total features selected based on EDA definitions: 25\n",
      "Shape after selecting EDA-based features: (40, 26)\n"
     ]
    }
   ],
   "source": [
    "df_selected, selected_feature_names_eda = select_eda_features(\n",
    "    df_engineered,\n",
    "    TOP_FEATURE_DEFINITIONS,\n",
    "    TOP_REGIONAL_FEATURES_DEF,\n",
    "    TOP_ASYMMETRY_FEATURES_DEF\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-preprocess-markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "step4-preprocess-func-def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df_selected, initial_feature_names, remove_outlier, outlier_idx, corr_threshold):\n",
    "    \"\"\"Preprocesses the selected data: outlier removal, correlation filtering, NaN handling.\"\"\"\n",
    "    print(\"\\n--- Task 1: Preprocessing --- \")\n",
    "    df_processed = df_selected.copy()\n",
    "    if remove_outlier:\n",
    "        if outlier_idx >= 0 and outlier_idx < len(df_processed):\n",
    "            print(f\"Removing outlier Sample # {outlier_idx + 1} (index {outlier_idx}).\")\n",
    "            df_processed = df_processed.drop(index=outlier_idx).reset_index(drop=True)\n",
    "        else: print(f\"Warning: Outlier index {outlier_idx} out of bounds. Skipping.\")\n",
    "\n",
    "    y = df_processed['target'].values\n",
    "    current_feature_names = [name for name in initial_feature_names if name in df_processed.columns]\n",
    "    X_raw = df_processed[current_feature_names].values\n",
    "    print(f\"Features before correlation filtering: {len(current_feature_names)}\")\n",
    "\n",
    "    X_filtered, feature_names_filtered = filter_correlated_features(X_raw, current_feature_names, threshold=corr_threshold)\n",
    "\n",
    "    if np.isnan(X_filtered).any() or np.isinf(X_filtered).any():\n",
    "        print(\"Handling NaN/Inf values in final features (replacing with 0).\")\n",
    "        X = np.nan_to_num(X_filtered, nan=0.0, posinf=np.finfo(np.float64).max, neginf=np.finfo(np.float64).min)\n",
    "    else: X = X_filtered\n",
    "    feature_names_final = feature_names_filtered\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "    target_counts = pd.Series(y).value_counts(normalize=True)\n",
    "    print(\"\\n--- Task 1: Data Summary (After Preprocessing for Modeling) ---\")\n",
    "    print(f\"Total samples for modeling: {n_samples}\")\n",
    "    print(f\"Features remaining for modeling: {n_features}\")\n",
    "    print(f\"Final features: {feature_names_final}\")\n",
    "    print(\"Target class distribution:\")\n",
    "    for class_val, proportion in target_counts.items(): print(f\"  Class {class_val}: {proportion:.1%}\")\n",
    "\n",
    "    if n_features == 0:\n",
    "        print(\"Error: No features remaining!\")\n",
    "        return None, None, None\n",
    "    return X, y, feature_names_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "step4-preprocess-execute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Task 1: Preprocessing --- \n",
      "Removing outlier Sample # 17 (index 16).\n",
      "Features before correlation filtering: 25\n",
      "Dropping 12 highly correlated features (threshold=0.93): ['delta50', 'delta52', 'theta42', 'theta50', 'theta52', 'central_right_delta_mean', 'parietal_right_delta_mean', 'central_right_theta_mean', 'temporal_right_delta_mean', 'parietal_right_theta_mean', 'temporal_left_theta_mean', 'temporal_right_theta_mean']\n",
      "\n",
      "--- Task 1: Data Summary (After Preprocessing for Modeling) ---\n",
      "Total samples for modeling: 39\n",
      "Features remaining for modeling: 13\n",
      "Final features: ['delta12', 'delta24', 'delta42', 'theta12', 'theta24', 'temporal_left_delta_mean', 'frontal_left_delta_mean', 'frontal_left_frontal_right_delta_asymmetry', 'central_left_central_right_delta_asymmetry', 'temporal_left_temporal_right_delta_asymmetry', 'parietal_left_parietal_right_delta_asymmetry', 'frontal_left_frontal_right_theta_asymmetry', 'central_left_central_right_theta_asymmetry']\n",
      "Target class distribution:\n",
      "  Class 0: 51.3%\n",
      "  Class 1: 48.7%\n"
     ]
    }
   ],
   "source": [
    "X, y, feature_names_final = preprocess_data(\n",
    "    df_selected,\n",
    "    selected_feature_names_eda,\n",
    "    REMOVE_OUTLIER_SAMPLE,\n",
    "    OUTLIER_SAMPLE_INDEX,\n",
    "    CORRELATION_THRESHOLD\n",
    ")\n",
    "if X is None:\n",
    "     raise SystemExit(\"Stopping: Preprocessing error (no features left).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-model-setup-markdown",
   "metadata": {},
   "source": [
    "## Modeling Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "step5-model-setup-func-def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_modeling(n_samples, n_splits_config, n_repeats_config, random_state):\n",
    "    \"\"\"Sets up CV strategy, scoring, pipelines, and parameter grids.\"\"\"\n",
    "    print(\"\\n--- Task 1: Model Training Setup ---\")\n",
    "    if n_samples < n_splits_config:\n",
    "         print(f\"Warning: Samples ({n_samples}) < CV splits ({n_splits_config}). Adjusting splits.\")\n",
    "         cv_splits = max(2, n_samples)\n",
    "    else: cv_splits = n_splits_config\n",
    "    cv_strategy = RepeatedStratifiedKFold(n_splits=cv_splits, n_repeats=n_repeats_config, random_state=random_state)\n",
    "    print(f\"Using Repeated Stratified K-Fold CV ({cv_splits} splits, {n_repeats_config} repeats)\")\n",
    "\n",
    "    precision_scorer = make_scorer(precision_score, pos_label=1, zero_division=0)\n",
    "    scoring = {'accuracy': 'accuracy', 'precision': precision_scorer}\n",
    "    refit_metric = 'accuracy'\n",
    "    print(f\"Metrics: Accuracy, Precision (pos_label=1, zero_division=0)\")\n",
    "    print(f\"GridSearch refit metric: {refit_metric}\")\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    pipelines = {}\n",
    "    param_grids = {}\n",
    "\n",
    "    # 1. Logistic Regression\n",
    "    pipelines['Logistic Regression'] = Pipeline([('scaler', scaler), ('classifier', LogisticRegression(random_state=random_state, max_iter=5000))])\n",
    "    param_grids['Logistic Regression'] = {\n",
    "        'classifier__penalty': ['l1', 'l2'], 'classifier__C': [0.001, 0.01, 0.1, 1, 10],\n",
    "        'classifier__solver': ['liblinear'], 'classifier__class_weight': ['balanced', None]\n",
    "    }\n",
    "    # 2. SVM (Linear Kernel)\n",
    "    pipelines['SVM (Linear Kernel)'] = Pipeline([('scaler', scaler), ('classifier', SVC(kernel='linear', probability=False, random_state=random_state, class_weight='balanced'))])\n",
    "    param_grids['SVM (Linear Kernel)'] = {'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "    # 3. SVM (RBF Kernel)\n",
    "    pipelines['SVM (RBF Kernel)'] = Pipeline([('scaler', scaler), ('classifier', SVC(kernel='rbf', probability=False, random_state=random_state, class_weight='balanced'))])\n",
    "    param_grids['SVM (RBF Kernel)'] = {\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'classifier__gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1.0]\n",
    "    }\n",
    "    # 4. SVM (Polynomial Kernel)\n",
    "    pipelines['SVM (Polynomial Kernel)'] = Pipeline([('scaler', scaler), ('classifier', SVC(kernel='poly', probability=False, random_state=random_state, class_weight='balanced'))])\n",
    "    param_grids['SVM (Polynomial Kernel)'] = {\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100], 'classifier__degree': [2, 3],\n",
    "        'classifier__gamma': ['scale', 'auto', 0.01, 0.1], 'classifier__coef0': [0, 1]\n",
    "    }\n",
    "    # 5. k-Nearest Neighbors (kNN)\n",
    "    max_k = min(n_samples // 2, 15)\n",
    "    knn_neighbors = [k for k in [1, 3, 5, 7, 9, 11, 13, 15] if k < n_samples and k <= max_k]\n",
    "    if not knn_neighbors: knn_neighbors = [1]\n",
    "    pipelines['k-Nearest Neighbors'] = Pipeline([('scaler', scaler), ('classifier', KNeighborsClassifier())])\n",
    "    param_grids['k-Nearest Neighbors'] = {\n",
    "        'classifier__n_neighbors': knn_neighbors,\n",
    "        'classifier__weights': ['uniform', 'distance'],\n",
    "        'classifier__metric': ['euclidean', 'manhattan']\n",
    "    }\n",
    "\n",
    "    return cv_strategy, scoring, pipelines, param_grids, refit_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "step5-model-setup-execute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Task 1: Model Training Setup ---\n",
      "Using Repeated Stratified K-Fold CV (5 splits, 5 repeats)\n",
      "Metrics: Accuracy, Precision (pos_label=1, zero_division=0)\n",
      "GridSearch refit metric: accuracy\n"
     ]
    }
   ],
   "source": [
    "cv_strategy, scoring, pipelines, param_grids, refit_metric = setup_modeling(\n",
    "    X.shape[0],\n",
    "    N_SPLITS_CV,\n",
    "    N_REPEATS_CV,\n",
    "    RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6-baseline-markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "step6-baseline-func-def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline(X, y, cv_strategy, scoring):\n",
    "    \"\"\"Runs the DummyClassifier baseline.\"\"\"\n",
    "    print(\"\\n--- Task 1: Baseline Performance (Predicting Most Frequent Class) ---\")\n",
    "    dummy_clf = DummyClassifier(strategy='most_frequent')\n",
    "    baseline_results = {}\n",
    "    try:\n",
    "        baseline_scores = cross_validate(dummy_clf, X, y, cv=cv_strategy, scoring=scoring)\n",
    "        baseline_results = {metric: (np.mean(scores), np.std(scores)) for metric, scores in baseline_scores.items() if metric.startswith('test_')}\n",
    "        print(f\"Baseline Mean Accuracy:  {baseline_results['test_accuracy'][0]:.4f} (+/- {baseline_results['test_accuracy'][1]:.4f})\")\n",
    "        print(f\"Baseline Mean Precision: {baseline_results['test_precision'][0]:.4f} (+/- {baseline_results['test_precision'][1]:.4f})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not compute baseline scores: {e}\")\n",
    "        baseline_results['error'] = str(e)\n",
    "    return baseline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "step6-baseline-execute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Task 1: Baseline Performance (Predicting Most Frequent Class) ---\n",
      "Baseline Mean Accuracy:  0.5143 (+/- 0.0286)\n",
      "Baseline Mean Precision: 0.0000 (+/- 0.0000)\n"
     ]
    }
   ],
   "source": [
    "baseline_results = run_baseline(X, y, cv_strategy, scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7-train-eval-markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Train & Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "step7-train-eval-func-def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(X, y, pipelines, param_grids, cv_strategy, scoring, refit_metric):\n",
    "    \"\"\"Runs GridSearchCV for all defined models.\"\"\"\n",
    "    print(f\"\\n--- Task 1: Training and Evaluating Models via GridSearchCV --- \")\n",
    "    results = {}\n",
    "    best_model_name = None\n",
    "    best_accuracy = -1\n",
    "\n",
    "    for name in pipelines:\n",
    "        print(f\"\\nTuning {name}...\")\n",
    "        pipeline = pipelines[name]\n",
    "        param_grid = param_grids[name]\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=pipeline, param_grid=param_grid, scoring=scoring,\n",
    "            refit=refit_metric, cv=cv_strategy, n_jobs=-1,\n",
    "            verbose=0, error_score='raise'\n",
    "        )\n",
    "        try:\n",
    "            grid_search.fit(X, y)\n",
    "            results[name] = {\n",
    "                'best_params': grid_search.best_params_,\n",
    "                'best_estimator': grid_search.best_estimator_\n",
    "            }\n",
    "            best_index = grid_search.best_index_\n",
    "            for metric in scoring.keys():\n",
    "                mean_score = grid_search.cv_results_[f'mean_test_{metric}'][best_index]\n",
    "                std_score = grid_search.cv_results_[f'std_test_{metric}'][best_index]\n",
    "                results[name][f'mean_test_{metric}'] = mean_score\n",
    "                results[name][f'std_test_{metric}'] = std_score\n",
    "\n",
    "            acc_mean = results[name]['mean_test_accuracy']\n",
    "            acc_std = results[name]['std_test_accuracy']\n",
    "            prec_mean = results[name]['mean_test_precision']\n",
    "            prec_std = results[name]['std_test_precision']\n",
    "            print(f\"  Best Mean Accuracy:  {acc_mean:.4f} (+/- {acc_std:.4f})\")\n",
    "            print(f\"  Best Mean Precision: {prec_mean:.4f} (+/- {prec_std:.4f})\")\n",
    "            print(f\"  Best Parameters: {results[name]['best_params']}\")\n",
    "\n",
    "            if acc_mean > best_accuracy:\n",
    "                best_accuracy = acc_mean\n",
    "                best_model_name = name\n",
    "        except Exception as e:\n",
    "             print(f\"  Error during GridSearchCV for {name}: {e}\")\n",
    "             results[name] = {'error': str(e)}\n",
    "\n",
    "    print(\"\\nTask 1 GridSearchCV Complete.\")\n",
    "    if best_model_name:\n",
    "        print(f\"Overall best model (based on mean accuracy): {best_model_name} ({best_accuracy:.4f})\")\n",
    "    else: print(\"No models completed successfully.\")\n",
    "    return results, best_model_name, best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "step7-train-eval-execute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Task 1: Training and Evaluating Models via GridSearchCV --- \n",
      "\n",
      "Tuning Logistic Regression...\n",
      "  Best Mean Accuracy:  0.5143 (+/- 0.0286)\n",
      "  Best Mean Precision: 0.0000 (+/- 0.0000)\n",
      "  Best Parameters: {'classifier__C': 0.001, 'classifier__class_weight': 'balanced', 'classifier__penalty': 'l1', 'classifier__solver': 'liblinear'}\n",
      "\n",
      "Tuning SVM (Linear Kernel)...\n",
      "  Best Mean Accuracy:  0.5214 (+/- 0.0825)\n",
      "  Best Mean Precision: 0.5067 (+/- 0.0557)\n",
      "  Best Parameters: {'classifier__C': 0.01}\n",
      "\n",
      "Tuning SVM (RBF Kernel)...\n",
      "  Best Mean Accuracy:  0.5329 (+/- 0.0703)\n",
      "  Best Mean Precision: 0.5103 (+/- 0.0462)\n",
      "  Best Parameters: {'classifier__C': 1, 'classifier__gamma': 0.001}\n",
      "\n",
      "Tuning SVM (Polynomial Kernel)...\n",
      "  Best Mean Accuracy:  0.6129 (+/- 0.1477)\n",
      "  Best Mean Precision: 0.6600 (+/- 0.2066)\n",
      "  Best Parameters: {'classifier__C': 100, 'classifier__coef0': 0, 'classifier__degree': 2, 'classifier__gamma': 0.1}\n",
      "\n",
      "Tuning k-Nearest Neighbors...\n",
      "  Best Mean Accuracy:  0.5071 (+/- 0.1005)\n",
      "  Best Mean Precision: 0.5118 (+/- 0.1409)\n",
      "  Best Parameters: {'classifier__metric': 'manhattan', 'classifier__n_neighbors': 13, 'classifier__weights': 'uniform'}\n",
      "\n",
      "Task 1 GridSearchCV Complete.\n",
      "Overall best model (based on mean accuracy): SVM (Polynomial Kernel) (0.6129)\n"
     ]
    }
   ],
   "source": [
    "model_results, best_model_name, best_accuracy = train_and_evaluate_models(\n",
    "    X, y, pipelines, param_grids, cv_strategy, scoring, refit_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step8-task2a-markdown",
   "metadata": {},
   "source": [
    "## Task 2 (Part A) - Analyze Original Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "step8-task2a-func-def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_task2_original_features(df_original, k_top_features, random_state):\n",
    "    \"\"\"\n",
    "    Performs Task 2 (UFS, RFE, PCA) on the original features of the dataset.\n",
    "    Returns a dictionary containing the top features identified by each method.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Task 2 (Part A): Analysis on Original 320 Features ---\")\n",
    "    results_task2a = defaultdict(list)\n",
    "\n",
    "    # --- Prepare Data ---\n",
    "    if 'target' not in df_original.columns:\n",
    "        print(\"Error: 'target' column not found.\")\n",
    "        results_task2a['error'] = \"Target column missing\"\n",
    "        return results_task2a\n",
    "\n",
    "    y_original = df_original['target'].values\n",
    "    X_original_df = df_original.drop('target', axis=1)\n",
    "    X_original = X_original_df.values\n",
    "    feature_names_original = X_original_df.columns.tolist()\n",
    "    n_samples, n_features = X_original.shape\n",
    "\n",
    "    print(f\"Analyzing {n_features} original features for {n_samples} samples.\")\n",
    "\n",
    "    # Handle NaNs/Infs\n",
    "    if np.isnan(X_original).any() or np.isinf(X_original).any():\n",
    "        print(\"Handling NaN/Inf values in original features (replacing with 0).\")\n",
    "        X_original = np.nan_to_num(X_original, nan=0.0, posinf=np.finfo(np.float64).max, neginf=np.finfo(np.float64).min)\n",
    "\n",
    "    # --- 2a. UFS ---\n",
    "    print(\"\\n--- 2a. UFS (f_classif) on Original Features ---\")\n",
    "    try:\n",
    "        scaler_ufs_orig = RobustScaler()\n",
    "        X_scaled_ufs_orig = scaler_ufs_orig.fit_transform(X_original)\n",
    "        k_ufs = min(k_top_features, n_features)\n",
    "        ufs_selector_orig = SelectKBest(score_func=f_classif, k=k_ufs)\n",
    "        ufs_selector_orig.fit(X_scaled_ufs_orig, y_original)\n",
    "        ufs_scores_all_orig = np.nan_to_num(ufs_selector_orig.scores_, nan=-np.inf)\n",
    "        ufs_sorted_indices_orig = np.argsort(ufs_scores_all_orig)[::-1]\n",
    "        ufs_top_indices_orig = ufs_sorted_indices_orig[:k_ufs]\n",
    "        ufs_features_orig = [feature_names_original[i] for i in ufs_top_indices_orig]\n",
    "        ufs_scores_orig = ufs_scores_all_orig[ufs_top_indices_orig]\n",
    "        print(f\"Top {k_ufs} original features (and scores) via UFS (f_classif):\")\n",
    "        for feature, score in zip(ufs_features_orig, ufs_scores_orig):\n",
    "            print(f\"  - {feature}: {score:.4f}\")\n",
    "        results_task2a['ufs_features'] = ufs_features_orig\n",
    "    except Exception as e:\n",
    "        print(f\"Error during UFS analysis on original features: {e}\")\n",
    "        results_task2a['ufs_features'] = [\"Error in UFS\"]\n",
    "\n",
    "    # --- 2b. RFE ---\n",
    "    print(\"\\n--- 2b. RFE on Original Features ---\")\n",
    "    try:\n",
    "        scaler_rfe_orig = RobustScaler()\n",
    "        X_scaled_rfe_orig = scaler_rfe_orig.fit_transform(X_original)\n",
    "        k_rfe = min(k_top_features, n_features)\n",
    "        estimator_rfe_orig = LogisticRegression(solver='liblinear', C=1.0, class_weight='balanced', random_state=random_state, max_iter=1000)\n",
    "        rfe_estimator_name = \"LogisticRegression(C=1.0)\"\n",
    "        print(f\"Using {rfe_estimator_name} for RFE (may take time)...\")\n",
    "        rfe_selector_orig = RFE(estimator=estimator_rfe_orig, n_features_to_select=k_rfe, step=0.1)\n",
    "        rfe_selector_orig.fit(X_scaled_rfe_orig, y_original)\n",
    "        rfe_indices_orig = rfe_selector_orig.get_support(indices=True)\n",
    "        rfe_features_orig = [feature_names_original[i] for i in rfe_indices_orig]\n",
    "        print(f\"Top {k_rfe} original features via RFE (using {rfe_estimator_name}):\")\n",
    "        for feature in rfe_features_orig:\n",
    "            print(f\"  - {feature}\")\n",
    "        results_task2a['rfe_features'] = rfe_features_orig\n",
    "    except Exception as e:\n",
    "        print(f\"Error during RFE analysis on original features: {e}\")\n",
    "        results_task2a['rfe_features'] = [\"Error in RFE\"]\n",
    "\n",
    "    # --- 2c. PCA ---\n",
    "    print(\"\\n--- 2c. PCA on Original Features ---\")\n",
    "    try:\n",
    "        scaler_pca_orig = StandardScaler()\n",
    "        X_scaled_pca_orig = scaler_pca_orig.fit_transform(X_original)\n",
    "        n_components_pca = min(n_features, n_samples)\n",
    "        pca_orig = PCA(n_components=n_components_pca, random_state=random_state)\n",
    "        pca_orig.fit(X_scaled_pca_orig)\n",
    "        print(f\"Explained variance ratio by top principal components:\")\n",
    "        cumulative_variance = 0\n",
    "        max_pcs_to_print = min(10, n_components_pca)\n",
    "        for i, ratio in enumerate(pca_orig.explained_variance_ratio_):\n",
    "            cumulative_variance += ratio\n",
    "            if i < max_pcs_to_print:\n",
    "                print(f\"  - PC{i+1}: {ratio:.4f} (Cumulative: {cumulative_variance:.4f})\")\n",
    "            elif i == max_pcs_to_print:\n",
    "                print(f\"      ... (remaining {n_components_pca - i} components explain {(1-cumulative_variance):.4f} variance)\")\n",
    "                break # Stop printing after max_pcs_to_print\n",
    "        k_pca_loadings = min(k_top_features, n_features)\n",
    "        print(f\"\\nTop {k_pca_loadings} original features based on absolute loading on PC1:\")\n",
    "        pc1_loadings_orig = pca_orig.components_[0]\n",
    "        feature_loadings_pc1_orig = sorted(zip(feature_names_original, np.abs(pc1_loadings_orig)), key=lambda x: x[1], reverse=True)\n",
    "        pca_features_pc1 = []\n",
    "        for feature, loading in feature_loadings_pc1_orig[:k_pca_loadings]:\n",
    "            print(f\"  - {feature}: {loading:.4f}\")\n",
    "            pca_features_pc1.append(feature)\n",
    "        results_task2a['pca_features_pc1'] = pca_features_pc1\n",
    "\n",
    "        if n_components_pca > 1:\n",
    "            print(f\"\\nTop {k_pca_loadings} original features based on absolute loading on PC2:\")\n",
    "            pc2_loadings_orig = pca_orig.components_[1]\n",
    "            feature_loadings_pc2_orig = sorted(zip(feature_names_original, np.abs(pc2_loadings_orig)), key=lambda x: x[1], reverse=True)\n",
    "            pca_features_pc2 = []\n",
    "            for feature, loading in feature_loadings_pc2_orig[:k_pca_loadings]:\n",
    "                print(f\"  - {feature}: {loading:.4f}\")\n",
    "                pca_features_pc2.append(feature)\n",
    "            results_task2a['pca_features_pc2'] = pca_features_pc2\n",
    "    except Exception as e:\n",
    "        print(f\"Error during PCA analysis on original features: {e}\")\n",
    "        results_task2a['pca_features_pc1'] = [\"Error in PCA\"]\n",
    "        results_task2a['pca_features_pc2'] = [\"Error in PCA\"]\n",
    "\n",
    "    print(\"\\n--- End of Task 2 (Part A) Analysis on Original Features ---\")\n",
    "    return dict(results_task2a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "step8-task2a-execute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Task 2 (Part A): Analysis on Original 320 Features ---\n",
      "Analyzing 320 original features for 40 samples.\n",
      "\n",
      "--- 2a. UFS (f_classif) on Original Features ---\n",
      "Top 5 original features (and scores) via UFS (f_classif):\n",
      "  - delta41: 1.8424\n",
      "  - delta51: 1.8316\n",
      "  - beta23: 1.7772\n",
      "  - theta23: 1.6022\n",
      "  - delta23: 1.4612\n",
      "\n",
      "--- 2b. RFE on Original Features ---\n",
      "Using LogisticRegression(C=1.0) for RFE (may take time)...\n",
      "Top 5 original features via RFE (using LogisticRegression(C=1.0)):\n",
      "  - delta40\n",
      "  - delta58\n",
      "  - delta60\n",
      "  - theta55\n",
      "  - gamma63\n",
      "\n",
      "--- 2c. PCA on Original Features ---\n",
      "Explained variance ratio by top principal components:\n",
      "  - PC1: 0.3477 (Cumulative: 0.3477)\n",
      "  - PC2: 0.1888 (Cumulative: 0.5365)\n",
      "  - PC3: 0.1067 (Cumulative: 0.6432)\n",
      "  - PC4: 0.0939 (Cumulative: 0.7371)\n",
      "  - PC5: 0.0632 (Cumulative: 0.8003)\n",
      "  - PC6: 0.0473 (Cumulative: 0.8476)\n",
      "  - PC7: 0.0299 (Cumulative: 0.8775)\n",
      "  - PC8: 0.0205 (Cumulative: 0.8980)\n",
      "  - PC9: 0.0189 (Cumulative: 0.9169)\n",
      "  - PC10: 0.0176 (Cumulative: 0.9345)\n",
      "      ... (remaining 30 components explain 0.0528 variance)\n",
      "\n",
      "Top 5 original features based on absolute loading on PC1:\n",
      "  - beta32: 0.0906\n",
      "  - beta27: 0.0906\n",
      "  - beta26: 0.0897\n",
      "  - beta30: 0.0894\n",
      "  - beta28: 0.0893\n",
      "\n",
      "Top 5 original features based on absolute loading on PC2:\n",
      "  - alpha44: 0.1000\n",
      "  - alpha42: 0.0980\n",
      "  - alpha46: 0.0969\n",
      "  - alpha16: 0.0939\n",
      "  - alpha45: 0.0936\n",
      "\n",
      "--- End of Task 2 (Part A) Analysis on Original Features ---\n"
     ]
    }
   ],
   "source": [
    "# Run Task 2 (Part A) using the initial raw dataframe\n",
    "analysis_results_task2a = run_task2_original_features(df_raw.copy(), K_TOP_FEATURES, RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step9-task2b-markdown",
   "metadata": {},
   "source": [
    "## Task 2 (Part B) - Analyze Task 1 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "step9-task2b-func-def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_task2_task1_features(X, y, feature_names, k_top_features, model_results, random_state):\n",
    "    \"\"\"Performs UFS, RFE, and PCA on the final features used in Task 1 models.\"\"\"\n",
    "    print(\"\\n--- Task 2 (Part B): Feature Selection Analysis on *Features Used in Task 1 Models* ---\")\n",
    "    analysis_results = defaultdict(list)\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    if n_features <= 0:\n",
    "        print(\"Skipping Task 2 (Part B) as no features were available for Task 1 modeling.\")\n",
    "        analysis_results['error'] = \"No features to analyze\"\n",
    "        return dict(analysis_results)\n",
    "\n",
    "    k_analyze = min(k_top_features, n_features)\n",
    "    print(f\"Analyzing top {k_analyze} features using UFS, RFE, and PCA on the {n_features} features used in Task 1.\")\n",
    "    print(f\"Features analyzed: {feature_names}\")\n",
    "\n",
    "    # --- 9a. UFS ---\n",
    "    print(\"\\n--- 9a. UFS (f_classif) on Task 1 Features ---\")\n",
    "    try:\n",
    "        scaler_ufs = RobustScaler()\n",
    "        X_scaled_ufs = scaler_ufs.fit_transform(X)\n",
    "        ufs_selector = SelectKBest(score_func=f_classif, k=k_analyze)\n",
    "        ufs_selector.fit(X_scaled_ufs, y)\n",
    "        ufs_scores_all = np.nan_to_num(ufs_selector.scores_, nan=-np.inf)\n",
    "        ufs_sorted_indices = np.argsort(ufs_scores_all)[::-1]\n",
    "        ufs_top_indices = ufs_sorted_indices[:k_analyze]\n",
    "        ufs_features = [feature_names[i] for i in ufs_top_indices]\n",
    "        ufs_scores = ufs_scores_all[ufs_top_indices]\n",
    "        print(f\"Top {k_analyze} Task 1 features (and scores) via UFS (f_classif):\")\n",
    "        for feature, score in zip(ufs_features, ufs_scores):\n",
    "            print(f\"  - {feature}: {score:.4f}\")\n",
    "        analysis_results['ufs_features'] = ufs_features\n",
    "    except Exception as e:\n",
    "        print(f\"Error during UFS analysis: {e}\")\n",
    "        analysis_results['ufs_features'] = [\"Error in UFS\"]\n",
    "\n",
    "    # --- 9b. RFE ---\n",
    "    print(\"\\n--- 9b. RFE on Task 1 Features ---\")\n",
    "    estimator_rfe = None\n",
    "    rfe_name = \"Default SVC(kernel='linear', C=1.0)\"\n",
    "    # Try to use best Linear SVM from Task 1\n",
    "    if 'SVM (Linear Kernel)' in model_results and 'error' not in model_results['SVM (Linear Kernel)']:\n",
    "        try:\n",
    "            params = {k.split('__')[1]: v for k, v in model_results['SVM (Linear Kernel)']['best_params'].items() if k.startswith('classifier__')}\n",
    "            params.setdefault('class_weight', 'balanced')\n",
    "            params.setdefault('probability', False)\n",
    "            params.setdefault('random_state', random_state)\n",
    "            estimator_rfe = SVC(kernel='linear', **params)\n",
    "            rfe_name = f\"Best Task 1 SVC(kernel='linear', C={params.get('C', '?')})\"\n",
    "            print(f\"Using {rfe_name} for RFE.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not use best Linear SVM params, using default. Error: {e}\")\n",
    "            estimator_rfe = SVC(kernel='linear', C=1.0, probability=False, random_state=random_state, class_weight='balanced')\n",
    "    else:\n",
    "        print(f\"Linear SVM results not found/had errors. Using {rfe_name} for RFE.\")\n",
    "        estimator_rfe = SVC(kernel='linear', C=1.0, probability=False, random_state=random_state, class_weight='balanced')\n",
    "\n",
    "    if estimator_rfe:\n",
    "        try:\n",
    "            scaler_rfe = RobustScaler()\n",
    "            X_scaled_rfe = scaler_rfe.fit_transform(X)\n",
    "            rfe_selector = RFE(estimator=estimator_rfe, n_features_to_select=k_analyze, step=1)\n",
    "            rfe_selector.fit(X_scaled_rfe, y)\n",
    "            rfe_indices = rfe_selector.get_support(indices=True)\n",
    "            rfe_features = [feature_names[i] for i in rfe_indices]\n",
    "            print(f\"Top {k_analyze} Task 1 features via RFE (using {rfe_name}):\")\n",
    "            for feature in rfe_features:\n",
    "                print(f\"  - {feature}\")\n",
    "            analysis_results['rfe_features'] = rfe_features\n",
    "        except Exception as e:\n",
    "            print(f\"Error during RFE analysis: {e}\")\n",
    "            analysis_results['rfe_features'] = [\"Error in RFE\"]\n",
    "    else:\n",
    "        analysis_results['rfe_features'] = [\"Error configuring RFE estimator\"]\n",
    "\n",
    "    # --- 9c. PCA ---\n",
    "    print(\"\\n--- 9c. PCA on Task 1 Features ---\")\n",
    "    try:\n",
    "        scaler_pca = StandardScaler()\n",
    "        X_scaled_pca = scaler_pca.fit_transform(X)\n",
    "        n_components_pca = min(n_features, n_samples)\n",
    "        pca = PCA(n_components=n_components_pca, random_state=random_state)\n",
    "        pca.fit(X_scaled_pca)\n",
    "        print(f\"Explained variance ratio by principal components (Task 1 features):\")\n",
    "        cumulative_variance_task1 = 0\n",
    "        for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "            cumulative_variance_task1 += ratio\n",
    "            print(f\"  - PC{i+1}: {ratio:.4f} (Cumulative: {cumulative_variance_task1:.4f})\")\n",
    "            # Stop printing details early if variance is mostly explained\n",
    "            if i+1 >= k_analyze and cumulative_variance_task1 > 0.95:\n",
    "                if i + 1 < n_components_pca: print(f\"      ... (remaining {n_components_pca - (i+1)} components)\")\n",
    "                break\n",
    "        print(f\"\\nTop {k_analyze} Task 1 features based on absolute loading on PC1:\")\n",
    "        pc1_loadings = pca.components_[0]\n",
    "        feature_loadings_pc1 = sorted(zip(feature_names, np.abs(pc1_loadings)), key=lambda x: x[1], reverse=True)\n",
    "        pca_features_pc1 = []\n",
    "        for feature, loading in feature_loadings_pc1[:k_analyze]:\n",
    "            print(f\"  - {feature}: {loading:.4f}\")\n",
    "            pca_features_pc1.append(feature)\n",
    "        analysis_results['pca_features_pc1'] = pca_features_pc1\n",
    "\n",
    "        if n_components_pca > 1:\n",
    "            print(f\"\\nTop {k_analyze} Task 1 features based on absolute loading on PC2:\")\n",
    "            pc2_loadings = pca.components_[1]\n",
    "            feature_loadings_pc2 = sorted(zip(feature_names, np.abs(pc2_loadings)), key=lambda x: x[1], reverse=True)\n",
    "            pca_features_pc2 = []\n",
    "            for feature, loading in feature_loadings_pc2[:k_analyze]:\n",
    "                print(f\"  - {feature}: {loading:.4f}\")\n",
    "                pca_features_pc2.append(feature)\n",
    "            analysis_results['pca_features_pc2'] = pca_features_pc2\n",
    "    except Exception as e:\n",
    "        print(f\"Error during PCA analysis: {e}\")\n",
    "        analysis_results['pca_features_pc1'] = [\"Error in PCA\"]\n",
    "        analysis_results['pca_features_pc2'] = [\"Error in PCA\"]\n",
    "\n",
    "    print(\"\\n--- End of Task 2 (Part B) Analysis on Task 1 Features ---\")\n",
    "    return dict(analysis_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "step9-task2b-execute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Task 2 (Part B): Feature Selection Analysis on *Features Used in Task 1 Models* ---\n",
      "Analyzing top 5 features using UFS, RFE, and PCA on the 13 features used in Task 1.\n",
      "Features analyzed: ['delta12', 'delta24', 'delta42', 'theta12', 'theta24', 'temporal_left_delta_mean', 'frontal_left_delta_mean', 'frontal_left_frontal_right_delta_asymmetry', 'central_left_central_right_delta_asymmetry', 'temporal_left_temporal_right_delta_asymmetry', 'parietal_left_parietal_right_delta_asymmetry', 'frontal_left_frontal_right_theta_asymmetry', 'central_left_central_right_theta_asymmetry']\n",
      "\n",
      "--- 9a. UFS (f_classif) on Task 1 Features ---\n",
      "Top 5 Task 1 features (and scores) via UFS (f_classif):\n",
      "  - temporal_left_delta_mean: 1.3219\n",
      "  - central_left_central_right_delta_asymmetry: 1.2246\n",
      "  - frontal_left_delta_mean: 1.1595\n",
      "  - delta42: 0.9475\n",
      "  - delta24: 0.6985\n",
      "\n",
      "--- 9b. RFE on Task 1 Features ---\n",
      "Using Best Task 1 SVC(kernel='linear', C=0.01) for RFE.\n",
      "Top 5 Task 1 features via RFE (using Best Task 1 SVC(kernel='linear', C=0.01)):\n",
      "  - delta42\n",
      "  - theta12\n",
      "  - theta24\n",
      "  - temporal_left_delta_mean\n",
      "  - frontal_left_delta_mean\n",
      "\n",
      "--- 9c. PCA on Task 1 Features ---\n",
      "Explained variance ratio by principal components (Task 1 features):\n",
      "  - PC1: 0.3631 (Cumulative: 0.3631)\n",
      "  - PC2: 0.2142 (Cumulative: 0.5773)\n",
      "  - PC3: 0.1337 (Cumulative: 0.7110)\n",
      "  - PC4: 0.1011 (Cumulative: 0.8121)\n",
      "  - PC5: 0.0656 (Cumulative: 0.8777)\n",
      "  - PC6: 0.0452 (Cumulative: 0.9229)\n",
      "  - PC7: 0.0323 (Cumulative: 0.9552)\n",
      "      ... (remaining 6 components)\n",
      "\n",
      "Top 5 Task 1 features based on absolute loading on PC1:\n",
      "  - central_left_central_right_delta_asymmetry: 0.3872\n",
      "  - delta24: 0.3797\n",
      "  - frontal_left_delta_mean: 0.3724\n",
      "  - frontal_left_frontal_right_theta_asymmetry: 0.3621\n",
      "  - parietal_left_parietal_right_delta_asymmetry: 0.3577\n",
      "\n",
      "Top 5 Task 1 features based on absolute loading on PC2:\n",
      "  - delta42: 0.4802\n",
      "  - theta24: 0.4451\n",
      "  - central_left_central_right_theta_asymmetry: 0.3097\n",
      "  - temporal_left_temporal_right_delta_asymmetry: 0.3021\n",
      "  - theta12: 0.2827\n",
      "\n",
      "--- End of Task 2 (Part B) Analysis on Task 1 Features ---\n"
     ]
    }
   ],
   "source": [
    "# Run Task 2 (Part B) using the final features from Task 1\n",
    "analysis_results_task2b = analyze_task2_task1_features(\n",
    "    X, y, feature_names_final, K_TOP_FEATURES, model_results, RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step10-summary-markdown",
   "metadata": {},
   "source": [
    "## Summarize Results & Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "step10-summary-func-def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(model_results, best_model, best_acc, task2a_results, task2b_results):\n",
    "    \"\"\"Prints the final summary of model performance and feature analysis.\"\"\"\n",
    "    print(\"\\n--- Final Summary & Reasoning --- \")\n",
    "    print(\"Task 1 Model Performance Summary:\")\n",
    "    results_list = []\n",
    "    for name, data in model_results.items():\n",
    "        if 'error' not in data:\n",
    "            row = {\n",
    "                'Model': name,\n",
    "                'Mean Accuracy': data.get('mean_test_accuracy', np.nan),\n",
    "                'Std Accuracy': data.get('std_test_accuracy', np.nan),\n",
    "                'Mean Precision': data.get('mean_test_precision', np.nan),\n",
    "                'Std Precision': data.get('std_test_precision', np.nan),\n",
    "                'Best Params': str(data.get('best_params', {}))\n",
    "            }\n",
    "            results_list.append(row)\n",
    "        else: print(f\"Model '{name}' failed: {data['error']}\")\n",
    "    if results_list:\n",
    "        results_df = pd.DataFrame(results_list).sort_values('Mean Accuracy', ascending=False)\n",
    "        float_cols = ['Mean Accuracy', 'Std Accuracy', 'Mean Precision', 'Std Precision']\n",
    "        for col in float_cols: results_df[col] = results_df[col].map('{:.4f}'.format)\n",
    "        try:\n",
    "            print(results_df.to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "        except ImportError:\n",
    "            print(results_df)\n",
    "    else: print(\"No successful model results to display.\")\n",
    "\n",
    "    print(\"\\nReasoning for Performance Differences (Example):\")\n",
    "    if best_model: print(f\"- Best model: '{best_model}' (Accuracy: {best_acc:.4f}).\") # Add more reasoning here based on model type\n",
    "    else: print(\"- No model completed successfully.\")\n",
    "\n",
    "    print(\"\\nTask 2 Feature Selection Summary:\")\n",
    "    print(\"  Part A (Original Features):\")\n",
    "    if 'error' not in task2a_results:\n",
    "        print(f\"  - UFS Top Features: {task2a_results.get('ufs_features', 'N/A')}\")\n",
    "        print(f\"  - RFE Top Features: {task2a_results.get('rfe_features', 'N/A')}\")\n",
    "        print(f\"  - PCA Top Features (PC1): {task2a_results.get('pca_features_pc1', 'N/A')}\")\n",
    "        if 'pca_features_pc2' in task2a_results:\n",
    "             print(f\"  - PCA Top Features (PC2): {task2a_results.get('pca_features_pc2', 'N/A')}\")\n",
    "    else: print(f\"  - Task 2(A) analysis failed: {task2a_results['error']}\")\n",
    "\n",
    "    print(\"\\n  Part B (Task 1 Features):\")\n",
    "    if 'error' not in task2b_results:\n",
    "        print(f\"  - UFS Top Features: {task2b_results.get('ufs_features', 'N/A')}\")\n",
    "        print(f\"  - RFE Top Features: {task2b_results.get('rfe_features', 'N/A')}\")\n",
    "        print(f\"  - PCA Top Features (PC1): {task2b_results.get('pca_features_pc1', 'N/A')}\")\n",
    "        if 'pca_features_pc2' in task2b_results:\n",
    "             print(f\"  - PCA Top Features (PC2): {task2b_results.get('pca_features_pc2', 'N/A')}\")\n",
    "    else: print(f\"  - Task 2(B) analysis failed: {task2b_results['error']}\")\n",
    "\n",
    "    print(\"\\nReasoning for Feature Selection Differences (Example):\")\n",
    "    print(\"- UFS (individual stats), RFE (model-based importance), PCA (variance).\")\n",
    "\n",
    "    print(\"\\n--- Complete --- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "step10-summary-execute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Summary & Reasoning --- \n",
      "Task 1 Model Performance Summary:\n",
      "                     Model Mean Accuracy Std Accuracy Mean Precision  \\\n",
      "3  SVM (Polynomial Kernel)        0.6129       0.1477         0.6600   \n",
      "2         SVM (RBF Kernel)        0.5329       0.0703         0.5103   \n",
      "1      SVM (Linear Kernel)        0.5214       0.0825         0.5067   \n",
      "0      Logistic Regression        0.5143       0.0286         0.0000   \n",
      "4      k-Nearest Neighbors        0.5071       0.1005         0.5118   \n",
      "\n",
      "  Std Precision                                        Best Params  \n",
      "3        0.2066  {'classifier__C': 100, 'classifier__coef0': 0,...  \n",
      "2        0.0462   {'classifier__C': 1, 'classifier__gamma': 0.001}  \n",
      "1        0.0557                            {'classifier__C': 0.01}  \n",
      "0        0.0000  {'classifier__C': 0.001, 'classifier__class_we...  \n",
      "4        0.1409  {'classifier__metric': 'manhattan', 'classifie...  \n",
      "\n",
      "Reasoning for Performance Differences (Example):\n",
      "- Best model: 'SVM (Polynomial Kernel)' (Accuracy: 0.6129).\n",
      "\n",
      "Task 2 Feature Selection Summary:\n",
      "  Part A (Original Features):\n",
      "  - UFS Top Features: ['delta41', 'delta51', 'beta23', 'theta23', 'delta23']\n",
      "  - RFE Top Features: ['delta40', 'delta58', 'delta60', 'theta55', 'gamma63']\n",
      "  - PCA Top Features (PC1): ['beta32', 'beta27', 'beta26', 'beta30', 'beta28']\n",
      "  - PCA Top Features (PC2): ['alpha44', 'alpha42', 'alpha46', 'alpha16', 'alpha45']\n",
      "\n",
      "  Part B (Task 1 Features):\n",
      "  - UFS Top Features: ['temporal_left_delta_mean', 'central_left_central_right_delta_asymmetry', 'frontal_left_delta_mean', 'delta42', 'delta24']\n",
      "  - RFE Top Features: ['delta42', 'theta12', 'theta24', 'temporal_left_delta_mean', 'frontal_left_delta_mean']\n",
      "  - PCA Top Features (PC1): ['central_left_central_right_delta_asymmetry', 'delta24', 'frontal_left_delta_mean', 'frontal_left_frontal_right_theta_asymmetry', 'parietal_left_parietal_right_delta_asymmetry']\n",
      "  - PCA Top Features (PC2): ['delta42', 'theta24', 'central_left_central_right_theta_asymmetry', 'temporal_left_temporal_right_delta_asymmetry', 'theta12']\n",
      "\n",
      "Reasoning for Feature Selection Differences (Example):\n",
      "- UFS (individual stats), RFE (model-based importance), PCA (variance).\n",
      "\n",
      "--- Complete --- \n"
     ]
    }
   ],
   "source": [
    "# Pass results from Task 1 modeling and both parts of Task 2\n",
    "summarize_results(\n",
    "    model_results,        \n",
    "    best_model_name,    \n",
    "    best_accuracy,      \n",
    "    analysis_results_task2a, \n",
    "    analysis_results_task2b  \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (toolbox)",
   "language": "python",
   "name": "toolbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
