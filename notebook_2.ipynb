{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d082a8d1-825b-4c02-bd5f-dbf0aeabdb35",
   "metadata": {},
   "source": [
    "# Brain Data Classification using Graph-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_01_markdown_imports",
   "metadata": {},
   "source": [
    "## 1. Imports, Configuration, Seeding & Device Setup\n",
    "\n",
    "This cell handles all necessary imports, sets up global configuration parameters, initializes random seeds for reproducibility, and selects the appropriate compute device (CPU or GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell_01_code_imports_config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Work\\anaconda3\\envs\\toolbox-torch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv, ChebConv, GATConv, global_mean_pool, global_add_pool, global_max_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import optuna\n",
    "import copy\n",
    "import os\n",
    "from collections import Counter\n",
    "import inspect\n",
    "\n",
    "# --- Configuration ---\n",
    "SEED = 42\n",
    "N_SPLITS = 5      # K for K-Fold Cross-Validation (used for final evaluation)\n",
    "EPOCHS = 150      # Max epochs for training (Early Stopping applies)\n",
    "PATIENCE = 20     # Patience for Early Stopping\n",
    "N_HPO_TRIALS = 50 # Number of trials per Optuna study\n",
    "N_TOP_MODELS = 5  # Number of best configurations to include in the final ensemble\n",
    "N_INTERNAL_CV_SPLITS = 3 # Number of folds for internal CV within HPO objective\n",
    "\n",
    "# Flags to control script execution\n",
    "RUN_HPO = False            # True for running Hyperparameter Optimization\n",
    "RUN_FINAL_TRAINING = True # True for trainning final models using best HPO params\n",
    "\n",
    "# Feature Set Selection\n",
    "# Options: 'basic' (5 wave bands), 'with_ratios' (5 bands + 2 ratios), 'full_domain' (5 bands + ratios + region/hemi OHE)\n",
    "FEATURE_SET = 'with_ratios'\n",
    "\n",
    "# Adjacency types to generate and potentially test\n",
    "ADJACENCY_TYPES = [\"KNN\", \"Threshold\", \"AnatomicalRegion\", \"CustomBrainNetwork\"]\n",
    "\n",
    "# --- Seed for Reproducibility ---\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    # Setting deterministic increases reproducibility\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# --- Device Selection ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_02_markdown_electrode_map",
   "metadata": {},
   "source": [
    "## 2. Electrode Mapping Utilities\n",
    "\n",
    "Defines functions to map EEG channel indices to standard 10-10 labels, group channels by brain region and hemisphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell_02_code_electrode_map",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_electrode_mapping_info():\n",
    "    \"\"\"\n",
    "    Provides mappings: 64ch index -> 10-10 label, 10-10 label -> region,\n",
    "    and generates region/hemisphere group lists needed for custom adjacency.\n",
    "    Uses a combination of Code A and Code B mapping logic.\n",
    "    Returns:\n",
    "        electrode_to_1010 (dict): Map from 1-64 index to 10-10 label string.\n",
    "        electrode_to_region (dict): Map from 0-63 index to simplified region string.\n",
    "        region_groups (dict): Dict mapping region names to lists of 0-63 indices.\n",
    "        hemisphere_groups (dict): Dict mapping 'left', 'right', 'midline' to 0-63 indices.\n",
    "    \"\"\"\n",
    "    # Map electrode numbers (1-64) to 10-10 positions\n",
    "    electrode_to_1010 = {\n",
    "        1: 'F10', 2: 'AF4', 3: 'F2', 4: 'FCz', 5: 'FP2', 6: 'Fz', 7: 'FC1', 8: 'AFz',\n",
    "        9: 'F1', 10: 'FP1', 11: 'AF3', 12: 'F3', 13: 'F5', 14: 'FC5', 15: 'FC3',\n",
    "        16: 'C1', 17: 'F9', 18: 'F7', 19: 'FT7', 20: 'C3', 21: 'CP1', 22: 'C5',\n",
    "        23: 'T9', 24: 'T7', 25: 'TP7', 26: 'CP5', 27: 'P5', 28: 'P3', 29: 'TP9',\n",
    "        30: 'P7', 31: 'P1', 32: 'P9', 33: 'PO3', 34: 'Pz', 35: 'O1', 36: 'POz',\n",
    "        37: 'Oz', 38: 'PO4', 39: 'O2', 40: 'P2', 41: 'CP2', 42: 'P4', 43: 'P10',\n",
    "        44: 'P8', 45: 'P6', 46: 'CP6', 47: 'TP10', 48: 'TP8', 49: 'C6', 50: 'C4',\n",
    "        51: 'C2', 52: 'T8', 53: 'FC4', 54: 'FC2', 55: 'T10', 56: 'FC6', 57: 'N/A',\n",
    "        58: 'F8', 59: 'F6', 60: 'F4', 61: 'N/A', 62: 'N/A', 63: 'N/A', 64: 'N/A'\n",
    "    }\n",
    "\n",
    "    # Define brain regions based on 10-10 system\n",
    "    frontal_labels = ['Fp1', 'Fp2', 'AF3', 'AF4', 'AF7', 'AF8', 'Fz', 'F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'AFz', 'FPz']\n",
    "    central_labels = ['Cz', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'FCz', 'FC1', 'FC2', 'FC3', 'FC4', 'FC5', 'FC6']\n",
    "    temporal_labels = ['T7', 'T8', 'T9', 'T10', 'FT7', 'FT8', 'FT9', 'FT10', 'TP7', 'TP8', 'TP9', 'TP10'] # Grouped T/FT/TP\n",
    "    parietal_labels = ['Pz', 'P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10', 'CPz', 'CP1', 'CP2', 'CP3', 'CP4', 'CP5', 'CP6']\n",
    "    occipital_labels = ['Oz', 'O1', 'O2', 'POz', 'PO3', 'PO4', 'PO7', 'PO8']\n",
    "\n",
    "    # Create mapping from electrode indices (0-63) to simplified brain regions\n",
    "    electrode_to_region = {}\n",
    "    regions_simplified = ['frontal', 'central', 'temporal', 'parietal', 'occipital']\n",
    "    region_label_map = {\n",
    "        'frontal': frontal_labels, 'central': central_labels, 'temporal': temporal_labels,\n",
    "        'parietal': parietal_labels, 'occipital': occipital_labels\n",
    "    }\n",
    "\n",
    "    for i in range(1, 65):\n",
    "        electrode_label = electrode_to_1010.get(i, 'N/A')\n",
    "        assigned_region = 'unknown'\n",
    "        if electrode_label != 'N/A':\n",
    "            # Find which simplified region contains this label (case-insensitive check might be needed)\n",
    "            for region_name, labels in region_label_map.items():\n",
    "                # Check variations like Fz vs FZ\n",
    "                if electrode_label.upper() in [l.upper() for l in labels]:\n",
    "                    assigned_region = region_name\n",
    "                    break\n",
    "        electrode_to_region[i-1] = assigned_region # Use 0-based index\n",
    "\n",
    "    # Group electrodes by region (0-based indices)\n",
    "    region_groups = {region_name: [] for region_name in regions_simplified + ['unknown']}\n",
    "    for idx, region_name in electrode_to_region.items():\n",
    "        if region_name in region_groups:\n",
    "             region_groups[region_name].append(idx)\n",
    "        else: # Should not happen with the init above, but safety check\n",
    "             region_groups['unknown'].append(idx)\n",
    "\n",
    "    # Hemisphere grouping (0-based indices)\n",
    "    left_labels = [label for label in electrode_to_1010.values()\n",
    "                   if label != 'N/A' and (label.endswith(('1', '3', '5', '7', '9')) or label.startswith(('Fp1', 'AF3', 'AF7', 'F1','F3','F5','F7','F9','FC1','FC3','FC5','FT7','FT9', 'C1','C3','C5','CP1','CP3','CP5','TP7','TP9','P1','P3','P5','P7','P9','PO3','PO7','O1')))] # More explicit L/R\n",
    "    right_labels = [label for label in electrode_to_1010.values()\n",
    "                    if label != 'N/A' and (label.endswith(('2', '4', '6', '8', '0')) or label.startswith(('Fp2','AF4','AF8','F2','F4','F6','F8','F10','FC2','FC4','FC6','FT8','FT10','C2','C4','C6','CP2','CP4','CP6','TP8','TP10','P2','P4','P6','P8','P10','PO4','PO8','O2')))] # 0 for F10/T10 etc.\n",
    "    midline_labels = [label for label in electrode_to_1010.values() if label != 'N/A' and label.endswith('z')]\n",
    "\n",
    "    left_electrodes = [i-1 for i in range(1, 65) if electrode_to_1010.get(i, 'N/A') in left_labels]\n",
    "    right_electrodes = [i-1 for i in range(1, 65) if electrode_to_1010.get(i, 'N/A') in right_labels]\n",
    "    midline_electrodes = [i-1 for i in range(1, 65) if electrode_to_1010.get(i, 'N/A') in midline_labels]\n",
    "\n",
    "    hemisphere_groups = {\n",
    "        'left': left_electrodes,\n",
    "        'right': right_electrodes,\n",
    "        'midline': midline_electrodes\n",
    "    }\n",
    "\n",
    "\n",
    "    return electrode_to_1010, electrode_to_region, region_groups, hemisphere_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_03_markdown_data_load",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Feature Engineering\n",
    "\n",
    "Loads the EEG data and channel coordinates. Preprocesses features using RobustScaler and adds engineered features (e.g., band ratios) based on the `FEATURE_SET` configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell_03_code_data_load",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(feature_set_config):\n",
    "    \"\"\"\n",
    "    Load EEG data and channel coordinates.\n",
    "    Preprocess basic wave features using RobustScaler.\n",
    "\n",
    "    Args:\n",
    "        feature_set_config (str): 'basic', 'with_ratios', 'full_domain'\n",
    "\n",
    "    Returns:\n",
    "        X_node (np.ndarray): Node features (n_samples, n_nodes, n_features)\n",
    "        y (np.ndarray): Target labels (n_samples,)\n",
    "        channel_coords (pd.DataFrame): DataFrame with channel coordinates.\n",
    "        electrode_mapping_info (dict): Contains dictionaries from get_electrode_mapping_info().\n",
    "    \"\"\"\n",
    "    print(f\"Loading data and preparing feature set: '{feature_set_config}'\")\n",
    "    eeg_data = pd.read_csv('EEG data - Sheet1.csv', index_col=0)\n",
    "    channel_coords = pd.read_csv('3d_channel_coord.csv')\n",
    "\n",
    "    # Get electrode mapping info needed for feature engineering and custom graph\n",
    "    electrode_to_1010, electrode_to_region, region_groups, hemisphere_groups = get_electrode_mapping_info()\n",
    "    electrode_mapping_info = {\n",
    "        \"electrode_to_1010\": electrode_to_1010,\n",
    "        \"electrode_to_region\": electrode_to_region,\n",
    "        \"region_groups\": region_groups,\n",
    "        \"hemisphere_groups\": hemisphere_groups\n",
    "    }\n",
    "\n",
    "\n",
    "    y = eeg_data['target'].values\n",
    "    wave_types = ['alpha', 'beta', 'delta', 'theta', 'gamma']\n",
    "    n_channels = 64\n",
    "    n_samples = len(eeg_data)\n",
    "\n",
    "    # --- 1. Extract and Scale Basic Wave Features ---\n",
    "    X_by_wave_raw = {}\n",
    "    X_by_wave_scaled = {}\n",
    "\n",
    "    for wave in wave_types:\n",
    "        wave_features_raw = np.zeros((n_samples, n_channels))\n",
    "        for ch in range(1, n_channels + 1):\n",
    "            col_name = f\"{wave}{ch}\"\n",
    "            if col_name in eeg_data.columns:\n",
    "                wave_features_raw[:, ch-1] = eeg_data[col_name].values\n",
    "            else:\n",
    "                 # Handle missing columns if necessary (e.g., fill with zeros or mean)\n",
    "                 print(f\"Warning: Column {col_name} not found in EEG data.\")\n",
    "        X_by_wave_raw[wave] = wave_features_raw\n",
    "\n",
    "        # Apply RobustScaler per wave type (scales features across samples and channels for that wave)\n",
    "        scaler = RobustScaler()\n",
    "        X_by_wave_scaled[wave] = scaler.fit_transform(X_by_wave_raw[wave])\n",
    "\n",
    "    # Basic node features: 5 scaled frequency bands\n",
    "    X_node_basic = np.zeros((n_samples, n_channels, len(wave_types)))\n",
    "    for i, wave in enumerate(wave_types):\n",
    "        X_node_basic[:, :, i] = X_by_wave_scaled[wave]\n",
    "\n",
    "    # --- 2. Adding Engineered Features ---\n",
    "    features_to_concat = [X_node_basic]\n",
    "\n",
    "    if feature_set_config == 'basic':\n",
    "        print(\"Using basic features (5 wave bands).\")\n",
    "\n",
    "    elif feature_set_config == 'with_ratios' or feature_set_config == 'full_domain':\n",
    "        print(\"Calculating ratio features...\")\n",
    "        ratio_features = np.zeros((n_samples, n_channels, 2))\n",
    "        # Calculate ratios using SCALED data for better numerical stability\n",
    "        epsilon = 1e-10\n",
    "        # Ensure denominators are safe\n",
    "        theta_scaled = X_by_wave_scaled['theta']\n",
    "        ratio_features[:, :, 0] = X_by_wave_scaled['alpha'] / (theta_scaled + epsilon)\n",
    "        ratio_features[:, :, 1] = X_by_wave_scaled['beta'] / (theta_scaled + epsilon)\n",
    "        features_to_concat.append(ratio_features)\n",
    "        print(f\"Added 2 ratio features.\")\n",
    "\n",
    "    if feature_set_config == 'full_domain':\n",
    "        print(\"Calculating region and hemisphere one-hot features...\")\n",
    "        # Region features (one-hot encoding based on electrode_to_region map)\n",
    "        regions_list = sorted([r for r in region_groups.keys() if r != 'unknown']) # Exclude unknown?\n",
    "        n_regions = len(regions_list)\n",
    "        region_map = {name: i for i, name in enumerate(regions_list)}\n",
    "        region_ohe_features = np.zeros((n_samples, n_channels, n_regions))\n",
    "        for sample_idx in range(n_samples):\n",
    "             for ch_idx in range(n_channels):\n",
    "                 region_name = electrode_to_region.get(ch_idx, 'unknown')\n",
    "                 if region_name in region_map:\n",
    "                     region_ohe_features[sample_idx, ch_idx, region_map[region_name]] = 1\n",
    "        features_to_concat.append(region_ohe_features)\n",
    "        print(f\"Added {n_regions} region OHE features.\")\n",
    "\n",
    "        # Hemisphere features (one-hot encoding based on hemisphere_groups map)\n",
    "        hemispheres_list = ['left', 'right', 'midline']\n",
    "        n_hemispheres = len(hemispheres_list)\n",
    "        hemisphere_ohe_features = np.zeros((n_samples, n_channels, n_hemispheres))\n",
    "        for sample_idx in range(n_samples):\n",
    "             for ch_idx in range(n_channels):\n",
    "                 if ch_idx in hemisphere_groups['left']:\n",
    "                     hemisphere_ohe_features[sample_idx, ch_idx, 0] = 1\n",
    "                 elif ch_idx in hemisphere_groups['right']:\n",
    "                     hemisphere_ohe_features[sample_idx, ch_idx, 1] = 1\n",
    "                 elif ch_idx in hemisphere_groups['midline']:\n",
    "                     hemisphere_ohe_features[sample_idx, ch_idx, 2] = 1\n",
    "        features_to_concat.append(hemisphere_ohe_features)\n",
    "        print(f\"Added {n_hemispheres} hemisphere OHE features.\")\n",
    "\n",
    "    # --- 3. Combine Features ---\n",
    "    X_node = np.concatenate(features_to_concat, axis=2)\n",
    "    print(f\"Final node feature shape: {X_node.shape}\") # (n_samples, n_nodes, n_features)\n",
    "\n",
    "    # Basic check for NaN/Inf values\n",
    "    if np.isnan(X_node).any() or np.isinf(X_node).any():\n",
    "        print(\"Warning: NaN or Inf values detected in final node features! Check scaling and ratios.\")\n",
    "        X_node = np.nan_to_num(X_node, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    return X_node, y, channel_coords, electrode_mapping_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_04_markdown_graph_construct",
   "metadata": {},
   "source": [
    "## 4. Graph Construction Utilities\n",
    "\n",
    "Defines functions to create adjacency matrices based on different strategies:\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Distance Threshold\n",
    "- Anatomical Regions (simplified)\n",
    "- Custom Brain Network (based on functional connectivity priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell_04_code_graph_construct",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_brain_adjacency(electrode_mapping_info, binary=True):\n",
    "    \"\"\"\n",
    "    Create adjacency matrix based on brain regions and functional networks.\n",
    "    Refined to be binary by default, reflecting connection presence.\n",
    "\n",
    "    Args:\n",
    "        electrode_mapping_info (dict): Contains region_groups, hemisphere_groups, electrode_to_1010.\n",
    "        binary (bool): binary is always True, connections are 1/0. I removed possibility edge weights.\n",
    "\n",
    "    Returns:\n",
    "        adj_matrix (np.ndarray): The adjacency matrix.\n",
    "    \"\"\"\n",
    "    region_groups = electrode_mapping_info['region_groups']\n",
    "    hemisphere_groups = electrode_mapping_info['hemisphere_groups']\n",
    "    # Ensure electrode_to_1010 maps 1-based index to label\n",
    "    electrode_to_1010 = electrode_mapping_info['electrode_to_1010']\n",
    "    # Create reverse map: label -> 0-based index for easier lookup\n",
    "    label_to_index0 = {v: k-1 for k, v in electrode_to_1010.items() if v != 'N/A'}\n",
    "\n",
    "\n",
    "    n_nodes = 64\n",
    "    adj_matrix = np.zeros((n_nodes, n_nodes))\n",
    "\n",
    "    # Use binary weights (1.0 for connection)\n",
    "    intra_region_w = 1.0\n",
    "    fronto_parietal_w = 1.0\n",
    "    # Identifying TP electrodes:\n",
    "    tp_labels = ['TP7','TP8','TP9','TP10']\n",
    "    # Use the label_to_index0 map\n",
    "    tp_indices_0based = {label_to_index0.get(label) for label in tp_labels if label in label_to_index0}\n",
    "    # Remove None if a label wasn't found (shouldn't happen)\n",
    "    tp_indices_0based.discard(None)\n",
    "\n",
    "    occip_temporal_w = 1.0\n",
    "    midline_w = 1.0\n",
    "    homologous_w = 1.0\n",
    "\n",
    "    # 1. Connect electrodes within the same brain region (excluding 'unknown')\n",
    "    for region, electrodes in region_groups.items():\n",
    "        if region == 'unknown': continue\n",
    "        valid_electrodes = [e for e in electrodes if e < n_nodes] # Ensure index is within bounds\n",
    "        for i in valid_electrodes:\n",
    "            for j in valid_electrodes:\n",
    "                if i != j:\n",
    "                    adj_matrix[i, j] = max(adj_matrix[i, j], intra_region_w) # Use max to avoid overwriting stronger links\n",
    "\n",
    "    # 2. Connect electrodes in functional networks\n",
    "\n",
    "    # Frontoparietal network\n",
    "    frontal_idx = [e for e in region_groups.get('frontal', []) if e < n_nodes]\n",
    "    parietal_idx = [e for e in region_groups.get('parietal', []) if e < n_nodes]\n",
    "    for i in frontal_idx:\n",
    "        for j in parietal_idx:\n",
    "             adj_matrix[i, j] = max(adj_matrix[i, j], fronto_parietal_w)\n",
    "             adj_matrix[j, i] = max(adj_matrix[j, i], fronto_parietal_w)\n",
    "\n",
    "    # Occipital-Temporal connections (simplified: connect all occipital to TP)\n",
    "    occipital_idx = [e for e in region_groups.get('occipital', []) if e < n_nodes]\n",
    "    temporal_idx = [e for e in region_groups.get('temporal', []) if e < n_nodes]\n",
    "    # Connecting occipital to temporo-parietal (TP*) sites\n",
    "    for i in occipital_idx:\n",
    "        for j in temporal_idx:\n",
    "            if j in tp_indices_0based: # Connect only to TP electrodes\n",
    "                 adj_matrix[i, j] = max(adj_matrix[i, j], occip_temporal_w)\n",
    "                 adj_matrix[j, i] = max(adj_matrix[j, i], occip_temporal_w)\n",
    "\n",
    "    # Midline network (connect all midline electrodes)\n",
    "    midline_idx = [e for e in hemisphere_groups.get('midline', []) if e < n_nodes]\n",
    "    for i in midline_idx:\n",
    "        for j in midline_idx:\n",
    "            if i != j:\n",
    "                adj_matrix[i, j] = max(adj_matrix[i, j], midline_w)\n",
    "\n",
    "\n",
    "    # 3. Connect homologous areas across hemispheres\n",
    "    left_idx = [e for e in hemisphere_groups.get('left', []) if e < n_nodes]\n",
    "    for i in left_idx:\n",
    "        label_i = electrode_to_1010.get(i + 1, '') # Get 1-based label\n",
    "        if not label_i or label_i == 'N/A': continue\n",
    "\n",
    "        # Find corresponding right label (handling numbers)\n",
    "        label_j = ''\n",
    "        # Handle T9/P9 -> T10/P10 specifically\n",
    "        if label_i in ['T9', 'P9', 'TP9']: # Add other potential 9s if needed\n",
    "            label_j = label_i[:-1] + '10'\n",
    "        elif label_i in ['F9']: # F9 doesn't have F10, maybe connects to F8?\n",
    "             label_j = 'F8'\n",
    "        elif label_i[-1].isdigit(): # Ends in 1, 3, 5, 7\n",
    "            num = int(label_i[-1])\n",
    "            if num % 2 != 0: # Is odd\n",
    "                label_j = label_i[:-1] + str(num + 1)\n",
    "\n",
    "        if label_j:\n",
    "            # Find the 0-based index j for label_j using the reverse map\n",
    "            found_j = label_to_index0.get(label_j, -1)\n",
    "\n",
    "            if found_j != -1 and found_j < n_nodes:\n",
    "                 adj_matrix[i, found_j] = max(adj_matrix[i, found_j], homologous_w)\n",
    "                 adj_matrix[found_j, i] = max(adj_matrix[found_j, i], homologous_w)\n",
    "\n",
    "\n",
    "    # Ensure no self-connections\n",
    "    np.fill_diagonal(adj_matrix, 0)\n",
    "    return adj_matrix\n",
    "\n",
    "\n",
    "def create_adjacency_matrices(channel_coords, electrode_mapping_info, k_knn=8, dist_threshold=0.2):\n",
    "    \"\"\"\n",
    "    Create multiple adjacency matrices based on coordinates and anatomical info.\n",
    "\n",
    "    Args:\n",
    "        channel_coords (pd.DataFrame): DataFrame with 'X', 'Y', 'Z' columns.\n",
    "        electrode_mapping_info (dict): Output from get_electrode_mapping_info().\n",
    "        k_knn (int): Number of neighbors for KNN graph.\n",
    "        dist_threshold (float): Normalized distance threshold for Threshold graph.\n",
    "\n",
    "    Returns:\n",
    "        adj_matrices (dict): Dictionary mapping adjacency type name to np.ndarray matrix.\n",
    "    \"\"\"\n",
    "    # Ensure coords are aligned with 0-63 indexing if channel_coords has labels\n",
    "    # Assuming channel_coords is already ordered 0-63 or doesn't have conflicting labels\n",
    "    coords = channel_coords[['X', 'Y', 'Z']].values\n",
    "    n_nodes = len(coords)\n",
    "    if n_nodes != 64:\n",
    "        print(f\"Warning: Expected 64 channels based on mapping, but found {n_nodes} coordinates. Using {n_nodes}.\")\n",
    "\n",
    "    # --- Calculate pairwise Euclidean distances (still needed for KNN and Threshold) ---\n",
    "    dist_matrix = np.zeros((n_nodes, n_nodes))\n",
    "    for i in range(n_nodes):\n",
    "        for j in range(i + 1, n_nodes):\n",
    "            dist = np.sqrt(np.sum((coords[i] - coords[j])**2))\n",
    "            dist_matrix[i, j] = dist_matrix[j, i] = dist\n",
    "\n",
    "    # --- Normalize distances (needed for Threshold) ---\n",
    "    max_dist = np.max(dist_matrix)\n",
    "    if max_dist > 0:\n",
    "        norm_dist_matrix = dist_matrix / max_dist\n",
    "    else:\n",
    "        norm_dist_matrix = dist_matrix # Avoid division by zero\n",
    "\n",
    "    adj_matrices = {}\n",
    "\n",
    "    # 1. KNN adjacency\n",
    "    knn_adj = np.zeros((n_nodes, n_nodes))\n",
    "    for i in range(n_nodes):\n",
    "        # Handle cases where k_knn >= n_nodes\n",
    "        k_actual = min(k_knn, n_nodes - 1)\n",
    "        if k_actual > 0:\n",
    "            neighbors = np.argsort(dist_matrix[i])[1:k_actual+1] # Exclude self\n",
    "            knn_adj[i, neighbors] = 1\n",
    "            knn_adj[neighbors, i] = 1 # Make symmetric\n",
    "    adj_matrices[\"KNN\"] = knn_adj\n",
    "\n",
    "    # 2. Distance threshold adjacency\n",
    "    threshold_adj = (norm_dist_matrix < dist_threshold).astype(np.float32)\n",
    "    np.fill_diagonal(threshold_adj, 0)\n",
    "    adj_matrices[\"Threshold\"] = threshold_adj\n",
    "\n",
    "    # 3. Anatomical Region-based adjacency (Connect nodes within the same *simplified* region)\n",
    "    region_adj = np.zeros((n_nodes, n_nodes))\n",
    "    region_groups = electrode_mapping_info['region_groups']\n",
    "    for region, electrodes in region_groups.items():\n",
    "        if region == 'unknown': continue\n",
    "        valid_electrodes = [e for e in electrodes if e < n_nodes]\n",
    "        for i in valid_electrodes:\n",
    "            for j in valid_electrodes:\n",
    "                 if i != j:\n",
    "                     region_adj[i, j] = 1 # Binary connection if in same region\n",
    "    adj_matrices[\"AnatomicalRegion\"] = region_adj\n",
    "\n",
    "    # 4. Custom Brain Network Adjacency\n",
    "    custom_adj = create_custom_brain_adjacency(electrode_mapping_info, binary=True)\n",
    "    adj_matrices[\"CustomBrainNetwork\"] = custom_adj\n",
    "\n",
    "    return adj_matrices # Return only the adjacency matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_05_markdown_pyg_dataset",
   "metadata": {},
   "source": [
    "## 5. PyTorch Geometric Dataset\n",
    "\n",
    "Defines a custom PyTorch Geometric `Dataset` class (`EEGGraphDataset`) to wrap the EEG data samples. Includes a helper function (`construct_graph_dataset`) to create instances of this dataset, handling the conversion of adjacency matrices to `edge_index` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell_05_code_pyg_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "# 4. PYTORCH GEOMETRIC DATASET \n",
    "#--------------------------------\n",
    "\n",
    "class EEGGraphDataset(Dataset):\n",
    "    \"\"\"PyTorch Geometric Dataset for EEG graph samples (no edge attributes).\"\"\"\n",
    "    def __init__(self, X_node, y, edge_index):\n",
    "        super().__init__()\n",
    "        self.X_node = torch.tensor(X_node, dtype=torch.float)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "        self.edge_index = edge_index # Should be a single tensor shared across graphs\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def get(self, idx):\n",
    "        x = self.X_node[idx]\n",
    "        target = self.y[idx]\n",
    "\n",
    "        # Create Data object for the sample (no edge_attr)\n",
    "        data = Data(x=x, edge_index=self.edge_index, y=target)\n",
    "        return data\n",
    "\n",
    "def construct_graph_dataset(X_node, y, adj_matrix):\n",
    "    \"\"\"\n",
    "    Construct PyTorch Geometric Dataset instance (no edge weights).\n",
    "\n",
    "    Args:\n",
    "        X_node (np.ndarray): Node features (n_samples, n_nodes, n_features).\n",
    "        y (np.ndarray): Target labels (n_samples,).\n",
    "        adj_matrix (np.ndarray): Adjacency matrix for the graph structure.\n",
    "\n",
    "    Returns:\n",
    "        EEGGraphDataset: The constructed dataset object.\n",
    "    \"\"\"\n",
    "    edge_index = torch.tensor(np.array(np.where(adj_matrix > 0)), dtype=torch.long)\n",
    "\n",
    "    pyg_dataset = EEGGraphDataset(X_node, y, edge_index)\n",
    "    return pyg_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_06_markdown_models",
   "metadata": {},
   "source": [
    "## 6. Model Architectures\n",
    "\n",
    "Defines the Graph Neural Network architectures to be tested:\n",
    "- `SimplifiedGNN`: Basic GCN-based model.\n",
    "- `SpectralGNN`: ChebConv-based model (spectral graph convolution).\n",
    "- `SimpleGAT`: Graph Attention Network (GAT)-based model.\n",
    "Includes options for batch normalization and different global pooling strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell_06_code_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# 5. MODEL ARCHITECTURES (Optional BatchNorm, Flexible Pooling, No Edge Weights)\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "def get_pooling_layer(pooling_type):\n",
    "    \"\"\"Returns the global pooling function based on type string.\"\"\"\n",
    "    if pooling_type == 'mean':\n",
    "        return global_mean_pool\n",
    "    elif pooling_type == 'add':\n",
    "        return global_add_pool\n",
    "    elif pooling_type == 'max':\n",
    "        return global_max_pool\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported pooling type: {pooling_type}\")\n",
    "\n",
    "class SimplifiedGNN(torch.nn.Module):\n",
    "    \"\"\" GCN-based model with optional BatchNorm. No edge weights. \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels=32, out_channels=2, dropout_rate=0.5, pooling_type='mean', use_batch_norm=False):\n",
    "        super().__init__()\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        if self.use_batch_norm:\n",
    "             self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        if self.use_batch_norm:\n",
    "            self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.pool = get_pooling_layer(pooling_type)\n",
    "        self.out = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        if self.use_batch_norm:\n",
    "             x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        if self.use_batch_norm:\n",
    "             x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x) # Dropout after second conv+activation\n",
    "\n",
    "        x = self.pool(x, batch)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class SpectralGNN(torch.nn.Module):\n",
    "    \"\"\" ChebConv-based model\"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels=32, out_channels=2, K=3, dropout_rate=0.5, pooling_type='mean', use_batch_norm=False):\n",
    "        super().__init__()\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.conv1 = ChebConv(in_channels, hidden_channels, K=K)\n",
    "        if self.use_batch_norm:\n",
    "             self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = ChebConv(hidden_channels, hidden_channels, K=K)\n",
    "        if self.use_batch_norm:\n",
    "            self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.pool = get_pooling_layer(pooling_type)\n",
    "        self.out = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index, batch=batch) # Pass batch if needed by ChebConv version\n",
    "        if self.use_batch_norm:\n",
    "             x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index, batch=batch) # Pass batch if needed by ChebConv version\n",
    "        if self.use_batch_norm:\n",
    "             x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.pool(x, batch)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class SimpleGAT(torch.nn.Module):\n",
    "    \"\"\" GAT-based model with optional BatchNorm. No edge weights. \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels=32, out_channels=2, heads=4, dropout_rate=0.5, pooling_type='mean', use_batch_norm=False):\n",
    "        super().__init__()\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        # Note: GAT output features are hidden_channels * heads for the first layer if concat=True\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout_rate) # Dropout within GATConv\n",
    "        if self.use_batch_norm:\n",
    "             # BN usually applied *after* convolution, *before* non-linearity\n",
    "             self.bn1 = nn.BatchNorm1d(hidden_channels * heads)\n",
    "\n",
    "        # Output layer (using 1 head and concat=False)\n",
    "        # Input channels for conv2 is hidden_channels * heads\n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=1, concat=False, dropout=dropout_rate)\n",
    "        if self.use_batch_norm:\n",
    "            self.bn2 = nn.BatchNorm1d(hidden_channels) # Output is hidden_channels\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate) # Separate dropout layer\n",
    "        self.pool = get_pooling_layer(pooling_type)\n",
    "        self.out = nn.Linear(hidden_channels, out_channels) # Output depends on conv2's output size\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # GATConv call without edge_attr\n",
    "        x = self.conv1(x, edge_index)\n",
    "        if self.use_batch_norm:\n",
    "             x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        # Dropout can be applied here (after activation) if not relying solely on GAT's internal dropout\n",
    "\n",
    "        # GATConv call\n",
    "        x = self.conv2(x, edge_index)\n",
    "        if self.use_batch_norm:\n",
    "             x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.pool(x, batch)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"SimplifiedGNN\": SimplifiedGNN,\n",
    "    \"SpectralGNN\": SpectralGNN,\n",
    "    \"SimpleGAT\": SimpleGAT\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_07_markdown_train_eval",
   "metadata": {},
   "source": [
    "## 7. Training & Evaluation Core Function\n",
    "\n",
    "Defines the `train_evaluate_single_model` function, which encapsulates the logic for training one epoch, evaluating on a validation set, implementing early stopping based on validation accuracy, and optionally interacting with Optuna for pruning during hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell_07_code_train_eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------\n",
    "# 6. TRAINING & EVALUATION CORE FUNCTION \n",
    "#----------------------------------------\n",
    "\n",
    "def train_evaluate_single_model(model, train_loader, val_loader, optimizer, criterion,\n",
    "                                device, epochs=100, patience=15, trial=None):\n",
    "    \"\"\"Trains and validates a single model instance with early stopping (no edge weights).\"\"\"\n",
    "    model.to(device)\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "    train_losses, val_losses, val_accs = [], [], []\n",
    "\n",
    "    # --- Determine number of samples for loss normalization ---\n",
    "    # Handle Subset datasets correctly for length\n",
    "    # Access the underlying dataset if it's a Subset\n",
    "    train_dataset_obj = train_loader.dataset.dataset if isinstance(train_loader.dataset, torch.utils.data.Subset) else train_loader.dataset\n",
    "    val_dataset_obj = val_loader.dataset.dataset if isinstance(val_loader.dataset, torch.utils.data.Subset) else val_loader.dataset\n",
    "    # Use the indices if it's a Subset\n",
    "    len_train_dataset = len(train_loader.dataset.indices) if isinstance(train_loader.dataset, torch.utils.data.Subset) else len(train_loader.dataset)\n",
    "    len_val_dataset = len(val_loader.dataset.indices) if isinstance(val_loader.dataset, torch.utils.data.Subset) else len(val_loader.dataset)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # Model call without edge_attr\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            # Ensure target shape matches output shape for CrossEntropyLoss\n",
    "            # Output shape: [batch_size, num_classes], Target shape: [batch_size]\n",
    "            loss = criterion(out, data.y)\n",
    "            loss.backward()\n",
    "            # Gradient Clipping (can help stability)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item() * data.num_graphs # Use num_graphs in batch\n",
    "        # Normalize loss by total number of samples in the dataset\n",
    "        avg_train_loss = total_train_loss / len_train_dataset if len_train_dataset > 0 else 0\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                data = data.to(device)\n",
    "                # Model call without edge_attr\n",
    "                out = model(data.x, data.edge_index, data.batch)\n",
    "                loss = criterion(out, data.y)\n",
    "                total_val_loss += loss.item() * data.num_graphs\n",
    "                pred = out.argmax(dim=1)\n",
    "                correct += int((pred == data.y).sum())\n",
    "\n",
    "        avg_val_loss = total_val_loss / len_val_dataset if len_val_dataset > 0 else 0\n",
    "        val_acc = correct / len_val_dataset if len_val_dataset > 0 else 0\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        # --- Early Stopping & Best Model Saving ---\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            # Save model state dict on CPU to avoid GPU memory issues if many models are trained\n",
    "            best_model_state = copy.deepcopy({k: v.cpu() for k, v in model.state_dict().items()})\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epoch % 25 == 0: # Print progress periodically\n",
    "            print(f'  Epoch: {epoch:03d}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        # --- Optuna Pruning ---\n",
    "        if trial:\n",
    "            trial.report(val_acc, epoch)\n",
    "            if trial.should_prune():\n",
    "                print(f\"  Trial pruned at epoch {epoch}.\")\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"  Early stopping triggered after epoch {epoch}. Best Val Acc: {best_val_acc:.4f}\")\n",
    "            break\n",
    "\n",
    "    # Load the best model state before returning\n",
    "    if best_model_state:\n",
    "        # Ensure the model is on the correct device before loading state\n",
    "        model.to(device)\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best_model_state.items()})\n",
    "    else:\n",
    "        print(\"Warning: No best model state found (possibly due to immediate pruning or no improvement). Using last state.\")\n",
    "        # Ensure model is still on the correct device\n",
    "        model.to(device)\n",
    "\n",
    "    return best_val_acc, model # Return best validation accuracy achieved and the model with best weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_08_markdown_hpo",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Optimization (Optuna)\n",
    "\n",
    "Defines the `objective` function for Optuna, which performs internal cross-validation for a given set of hyperparameters to get a robust performance estimate. Also defines `run_hpo_studies` to manage the optimization process across different model architectures and adjacency matrix types, saving the best parameters found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell_08_code_hpo",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "# 7. OPTUNA HYPERPARAMETER OPTIMIZATION\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def objective(trial, model_name, adj_type, X_node, y, adj_matrices, device):\n",
    "    \"\"\"\n",
    "    Optuna objective function using internal k-fold CV.\n",
    "    Includes use_batch_norm hyperparameter. No edge weights.\n",
    "    \"\"\"\n",
    "    # --- Hyperparameter Sampling ---\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.2, 0.7)\n",
    "    hidden_channels = trial.suggest_categorical(\"hidden_channels\", [16, 32, 64])\n",
    "    pooling_type = trial.suggest_categorical(\"pooling_type\", ['mean', 'add', 'max'])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"AdamW\"])\n",
    "    use_batch_norm = trial.suggest_categorical(\"use_batch_norm\", [True, False])\n",
    "\n",
    "    # --- Model Specific Hyperparameters ---\n",
    "    model_init_params = {\n",
    "        # in_channels determined dynamically later\n",
    "        \"hidden_channels\": hidden_channels,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"pooling_type\": pooling_type,\n",
    "        \"use_batch_norm\": use_batch_norm, # Pass BN choice\n",
    "        \"out_channels\": 2\n",
    "    }\n",
    "    if model_name == \"SpectralGNN\":\n",
    "        model_init_params[\"K\"] = trial.suggest_int(\"K\", 2, 5)\n",
    "    if model_name == \"SimpleGAT\":\n",
    "        model_init_params[\"heads\"] = trial.suggest_categorical(\"heads\", [2, 4, 8])\n",
    "\n",
    "    # --- Internal Cross-Validation Loop ---\n",
    "    kf_internal = StratifiedKFold(n_splits=N_INTERNAL_CV_SPLITS, shuffle=True, random_state=SEED)\n",
    "    internal_fold_accuracies = []\n",
    "    adj_matrix = adj_matrices[adj_type]\n",
    "\n",
    "    # Construct dataset for this adjacency type (once outside internal loop)\n",
    "    full_dataset = construct_graph_dataset(X_node, y, adj_matrix)\n",
    "    # Dynamically determine in_channels from the actual data\n",
    "    in_channels = full_dataset.num_node_features\n",
    "    model_init_params[\"in_channels\"] = in_channels # Add to params dict\n",
    "\n",
    "    for fold_num, (train_idx, val_idx) in enumerate(kf_internal.split(np.zeros(len(y)), y)): # Use dummy X for split indices\n",
    "\n",
    "        train_dataset = torch.utils.data.Subset(full_dataset, train_idx)\n",
    "        val_dataset = torch.utils.data.Subset(full_dataset, val_idx)\n",
    "        # Use suggested batch_size\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True) # drop_last might help with BN issues on small last batches\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "        # --- Instantiate Model & Optimizer *INSIDE* internal loop ---\n",
    "        ModelClass = MODEL_CLASSES[model_name]\n",
    "        # Filter params to only those accepted by the model's __init__\n",
    "        valid_params = {k: v for k, v in model_init_params.items() if k in inspect.signature(ModelClass.__init__).parameters}\n",
    "        try:\n",
    "            model = ModelClass(**valid_params)\n",
    "        except TypeError as e:\n",
    "             print(f\"ERROR Instantiating {model_name} with params {valid_params}: {e}\")\n",
    "             return 0.0 # Penalize trial heavily\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        if optimizer_name == \"Adam\":\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif optimizer_name == \"AdamW\":\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # --- Train and Evaluate on this internal fold ---\n",
    "        try:\n",
    "            # Pass trial=None to prevent pruning within the internal CV loop itself\n",
    "            # Pruning decision will be based on the average score reported later\n",
    "            # Use global EPOCHS and PATIENCE\n",
    "            best_fold_val_acc, _ = train_evaluate_single_model(\n",
    "                model, train_loader, val_loader, optimizer, criterion, device,\n",
    "                epochs=EPOCHS, patience=PATIENCE, trial=None # Pass None for internal folds\n",
    "            )\n",
    "            internal_fold_accuracies.append(best_fold_val_acc)\n",
    "\n",
    "        # Catch pruning signal if it somehow propagates (shouldn't tho)\n",
    "        except optuna.TrialPruned:\n",
    "             print(f\"    Trial {trial.number} - Pruning detected unexpectedly in internal fold {fold_num+1}.\")\n",
    "             # Re-raise or handle as error for the trial\n",
    "             internal_fold_accuracies.append(0.0) # Penalize score\n",
    "             break # Stop internal CV for this trial\n",
    "        # Catch other errors during training/evaluation\n",
    "        except Exception as e:\n",
    "            print(f\"    Trial {trial.number} - ERROR during internal fold {fold_num+1}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc() # Print stack trace for debugging\n",
    "            internal_fold_accuracies.append(0.0) # Penalize errors\n",
    "\n",
    "    # --- Calculate Average Score ---\n",
    "    if not internal_fold_accuracies: # Handle case where all internal folds failed\n",
    "        avg_val_acc = 0.0\n",
    "    else:\n",
    "        avg_val_acc = np.mean(internal_fold_accuracies)\n",
    "\n",
    "    # --- Report Average Score to Optuna for Pruning ---\n",
    "    # Report the average value at a single step (e.g., step=1)\n",
    "    # Use the main trial object passed to the objective function here\n",
    "    trial.report(avg_val_acc, 1)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "\n",
    "    return avg_val_acc # Optuna maximizes this average value\n",
    "\n",
    "\n",
    "def run_hpo_studies(X_node, y, adj_matrices, device):\n",
    "    \"\"\"Runs Optuna studies for all model/adjacency combinations (no distance matrix).\"\"\"\n",
    "    all_best_params = {}\n",
    "    study_results = {}\n",
    "\n",
    "    # Create a directory for HPO results if it doesn't exist\n",
    "    hpo_dir = \"hpo_results\"\n",
    "    os.makedirs(hpo_dir, exist_ok=True)\n",
    "\n",
    "    # Use global ADJACENCY_TYPES and FEATURE_SET\n",
    "    for model_name in MODEL_CLASSES.keys():\n",
    "        for adj_type in ADJACENCY_TYPES:\n",
    "            if adj_type not in adj_matrices:\n",
    "                print(f\"Skipping HPO for adjacency type '{adj_type}' as it wasn't generated.\")\n",
    "                continue\n",
    "\n",
    "            study_name = f\"{model_name}_{adj_type}_feat_{FEATURE_SET}\" # Include feature set in name\n",
    "            print(f\"\\n--- Running HPO for: {study_name} ---\")\n",
    "\n",
    "            # Define the objective function with fixed args for this study (no norm_dist_matrix)\n",
    "            objective_func = lambda trial: objective(trial, model_name, adj_type, X_node, y, adj_matrices, device)\n",
    "\n",
    "            # Create and run the Optuna study\n",
    "            # Use global SEED\n",
    "            study = optuna.create_study(direction=\"maximize\", study_name=study_name,\n",
    "                                       pruner=optuna.pruners.MedianPruner(n_warmup_steps=5, n_min_trials=3),\n",
    "                                       sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "            try:\n",
    "                 # Use global N_HPO_TRIALS\n",
    "                 study.optimize(objective_func, n_trials=N_HPO_TRIALS, timeout=None)\n",
    "                 best_params = study.best_trial.params\n",
    "                 best_value = study.best_value\n",
    "            except Exception as e:\n",
    "                 print(f\"ERROR during HPO study {study_name}: {e}\")\n",
    "                 import traceback\n",
    "                 traceback.print_exc()\n",
    "                 best_params = {}\n",
    "                 best_value = 0.0\n",
    "\n",
    "            # Store results\n",
    "            all_best_params[study_name] = best_params\n",
    "            study_results[study_name] = {\"best_value\": best_value, \"best_params\": best_params}\n",
    "            print(f\"Best Validation Accuracy (Avg Internal CV) for {study_name}: {best_value:.4f}\")\n",
    "            print(f\"Best Params: {best_params}\")\n",
    "\n",
    "            # Save study results incrementally\n",
    "            results_filename = os.path.join(hpo_dir, f'hpo_results_{study_name}.json')\n",
    "            try:\n",
    "                with open(results_filename, 'w') as f:\n",
    "                    json.dump(study_results[study_name], f, indent=4)\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving HPO results for {study_name} to {results_filename}: {e}\")\n",
    "\n",
    "\n",
    "    # Save all best parameters together\n",
    "    all_params_filename = os.path.join(hpo_dir, f'best_hpo_params_all_feat_{FEATURE_SET}.json')\n",
    "    try:\n",
    "        with open(all_params_filename, 'w') as f:\n",
    "            json.dump(all_best_params, f, indent=4)\n",
    "        print(f\"\\n--- HPO Complete. Best parameters saved to '{all_params_filename}' ---\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving combined HPO parameters to {all_params_filename}: {e}\")\n",
    "\n",
    "    return all_best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_09_markdown_final_train",
   "metadata": {},
   "source": [
    "## 9. Final Model Training & Ensemble Evaluation\n",
    "\n",
    "Defines functions for the final stage:\n",
    "- `train_final_models_cv`: Loads the best hyperparameters found by Optuna, selects the top N configurations, and trains each using full K-fold cross-validation on the entire dataset. Stores trained models and out-of-fold (OOF) predictions.\n",
    "- `evaluate_ensemble_cv`: Takes the OOF predictions from the trained models, performs majority voting to create an ensemble prediction, and evaluates the ensemble's performance using metrics like accuracy, F1-score, classification report, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell_09_code_final_train",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "# 8. FINAL MODEL TRAINING WITH CROSS-VALIDATION\n",
    "#---------------------------------------------\n",
    "\n",
    "def train_final_models_cv(best_hpo_params_path, X_node, y, adj_matrices, device):\n",
    "    \"\"\"\n",
    "    Trains the final models using best HPO params and full CV.\n",
    "    Ranks configurations based on their saved robust objective score from HPO.\n",
    "    \"\"\"\n",
    "    # Use global N_SPLITS and SEED\n",
    "    kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    trained_models = {} # Store models for ensemble {config_name: [model_fold1_state_dict, model_fold2_state_dict,...]}\n",
    "    oof_predictions = {} # Store out-of-fold predictions {config_name: np.array([...])}\n",
    "    oof_indices = {}     # Store indices corresponding to OOF predictions {config_name: np.array([...])}\n",
    "    cv_scores = {}       # Store CV scores per config {config_name: [acc_fold1, ...]}\n",
    "\n",
    "    # --- Load and Rank Configurations ---\n",
    "    try:\n",
    "        with open(best_hpo_params_path, 'r') as f:\n",
    "            best_hpo_params_all = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: HPO parameters file not found at '{best_hpo_params_path}'. Run HPO first.\")\n",
    "        return None, None, None, None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON from HPO parameters file '{best_hpo_params_path}': {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Extract HPO scores (best_value) to rank configurations\n",
    "    config_scores = {}\n",
    "    hpo_results_dir = os.path.dirname(best_hpo_params_path)\n",
    "    if not hpo_results_dir:\n",
    "        hpo_results_dir = \".\"\n",
    "\n",
    "    for config_name in best_hpo_params_all.keys():\n",
    "        individual_result_path = os.path.join(hpo_results_dir, f'hpo_results_{config_name}.json')\n",
    "        try:\n",
    "             with open(individual_result_path, 'r') as f:\n",
    "                 result_data = json.load(f)\n",
    "                 config_scores[config_name] = result_data.get('best_value', 0.0) # Default to 0 if score missing\n",
    "        except FileNotFoundError:\n",
    "             print(f\"Warning: Individual HPO result file not found for {config_name} at '{individual_result_path}'. Cannot rank accurately. Assigning score 0.\")\n",
    "             config_scores[config_name] = 0.0\n",
    "        except json.JSONDecodeError as e:\n",
    "             print(f\"Error decoding JSON from individual HPO result file '{individual_result_path}': {e}. Assigning score 0.\")\n",
    "             config_scores[config_name] = 0.0\n",
    "        except Exception as e:\n",
    "             print(f\"Error loading score for {config_name}: {e}. Assigning score 0.\")\n",
    "             config_scores[config_name] = 0.0\n",
    "\n",
    "\n",
    "    # Sort configurations by their validation score from HPO\n",
    "    sorted_configs = sorted(config_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    # Filter out configs with score 0 (likely errors during HPO) before selecting top N\n",
    "    valid_sorted_configs = [ (name, score) for name, score in sorted_configs if score > 0 ]\n",
    "\n",
    "    if not valid_sorted_configs:\n",
    "         print(\"Error: No valid HPO configurations found with score > 0. Cannot proceed with final training.\")\n",
    "         return None, None, None, None\n",
    "\n",
    "    # Use global N_TOP_MODELS\n",
    "    num_to_select = min(N_TOP_MODELS, len(valid_sorted_configs))\n",
    "    top_configs = dict(valid_sorted_configs[:num_to_select])\n",
    "    print(f\"\\n--- Selected Top {num_to_select} Configurations for Final Training (Ranked by HPO Score) ---\")\n",
    "    for name, score in top_configs.items():\n",
    "        print(f\"- {name} (HPO Score: {score:.4f})\")\n",
    "\n",
    "    # Determine in_channels dynamically from the provided X_node\n",
    "    in_channels = X_node.shape[2]\n",
    "    # Use global FEATURE_SET\n",
    "    print(f\"Using input channels: {in_channels} (based on FEATURE_SET='{FEATURE_SET}')\")\n",
    "\n",
    "    # --- Train each top configuration across all CV folds ---\n",
    "    for config_name in top_configs.keys():\n",
    "        print(f\"\\n--- Training Final Model for: {config_name} ---\")\n",
    "        # --- Improved Parsing Logic ---\n",
    "        try:\n",
    "            # Split the config name based on the feature set marker '_feat_'\n",
    "            base_name, feature_suffix = config_name.split('_feat_', 1)\n",
    "\n",
    "            # Split the base name (ModelName_AdjType) at the first underscore\n",
    "            parts = base_name.split('_')\n",
    "            if len(parts) < 2:\n",
    "                raise ValueError(\"Base name does not contain ModelName_AdjType\")\n",
    "            model_name = parts[0]\n",
    "            adj_type = \"_\".join(parts[1:]) # Handle adj_types with underscores if any\n",
    "\n",
    "\n",
    "        except ValueError as e:\n",
    "            # Fallback if splitting fails\n",
    "            print(f\"ERROR: Could not parse config name '{config_name}' using expected format 'ModelName_AdjType_feat_FeatureSet'. Error: {e}. Skipping.\")\n",
    "            # Assign invalid values to ensure skipping in subsequent checks\n",
    "            model_name = \"PARSE_ERROR\"\n",
    "            adj_type = \"PARSE_ERROR\"\n",
    "\n",
    "        if model_name not in MODEL_CLASSES:\n",
    "             print(f\"ERROR: Model name '{model_name}' from config '{config_name}' not found in MODEL_CLASSES. Skipping.\")\n",
    "             continue\n",
    "        if adj_type not in adj_matrices:\n",
    "             print(f\"ERROR: Adjacency type '{adj_type}' from config '{config_name}' not found in generated matrices. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        params = best_hpo_params_all[config_name] # Get the parameters for the current top config\n",
    "\n",
    "        fold_models_states = []\n",
    "        fold_oof_preds = []\n",
    "        fold_oof_indices = []\n",
    "        fold_scores = []\n",
    "\n",
    "        adj_matrix = adj_matrices[adj_type]\n",
    "        # Construct dataset for this fold using the appropriate adjacency matrix\n",
    "        full_dataset = construct_graph_dataset(X_node, y, adj_matrix)\n",
    "        # Double check in_channels matches dataset\n",
    "        if full_dataset.num_node_features != in_channels:\n",
    "             print(f\"ERROR: Mismatch between expected in_channels ({in_channels}) and dataset features ({full_dataset.num_node_features}) for {config_name}. Skipping.\")\n",
    "             continue\n",
    "\n",
    "\n",
    "        for fold, (train_idx, test_idx) in enumerate(kf.split(np.zeros(len(y)), y)): # Use dummy X for split\n",
    "            print(f\"  Fold {fold+1}/{N_SPLITS}\")\n",
    "\n",
    "            train_dataset = torch.utils.data.Subset(full_dataset, train_idx)\n",
    "            test_dataset = torch.utils.data.Subset(full_dataset, test_idx) # Test set for this fold\n",
    "\n",
    "            batch_size = params.get('batch_size', 8) # Default batch size if not in HPO params\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "            # Instantiate Model\n",
    "            ModelClass = MODEL_CLASSES[model_name]\n",
    "            # Prepare parameters for model instantiation\n",
    "            model_init_params = {\n",
    "                \"in_channels\": in_channels, # Use dynamically determined value\n",
    "                \"out_channels\": 2 # Assuming binary classification\n",
    "            }\n",
    "            # Add HPO params relevant to the model's __init__\n",
    "            init_signature = inspect.signature(ModelClass.__init__)\n",
    "            for k, v in params.items():\n",
    "                 if k in init_signature.parameters:\n",
    "                     model_init_params[k] = v\n",
    "\n",
    "             # Filter again to be sure\n",
    "            valid_params = {k: v for k, v in model_init_params.items() if k in init_signature.parameters}\n",
    "\n",
    "            try:\n",
    "                model = ModelClass(**valid_params)\n",
    "            except TypeError as e:\n",
    "                print(f\"  ERROR: Could not instantiate model {model_name} for fold {fold+1}. Config: {valid_params}. Error: {e}\")\n",
    "                fold_scores.append(0.0) # Record failure\n",
    "                continue # Skip to next fold\n",
    "\n",
    "            model.to(device)\n",
    "\n",
    "            # Instantiate Optimizer\n",
    "            optimizer_name = params.get('optimizer', 'Adam')\n",
    "            lr = params.get('lr', 0.001)\n",
    "            weight_decay = params.get('weight_decay', 0.0)\n",
    "\n",
    "            if optimizer_name == \"Adam\":\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "            elif optimizer_name == \"AdamW\":\n",
    "                optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "            else: # Default fallback\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            # Train model on this fold's train/val (using test_loader as val here for early stopping)\n",
    "            # Use global EPOCHS, PATIENCE\n",
    "            val_acc, trained_fold_model = train_evaluate_single_model(\n",
    "                model, train_loader, test_loader, optimizer, criterion, device,\n",
    "                epochs=EPOCHS, patience=PATIENCE, trial=None # No trial object for final train\n",
    "            )\n",
    "\n",
    "            # --- Evaluate on Test set for this fold (for OOF predictions) ---\n",
    "            trained_fold_model.eval()\n",
    "            preds = []\n",
    "            with torch.no_grad():\n",
    "                for data in test_loader:\n",
    "                    data = data.to(device)\n",
    "                    # Model call without edge_attr\n",
    "                    out = trained_fold_model(data.x, data.edge_index, data.batch)\n",
    "                    preds.extend(out.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "            # Use y[test_idx] for calculating fold accuracy\n",
    "            fold_test_acc = accuracy_score(y[test_idx], preds)\n",
    "            fold_scores.append(fold_test_acc) # Use test accuracy for the fold score\n",
    "            fold_oof_preds.extend(preds)\n",
    "            fold_oof_indices.extend(test_idx) # Store original indices\n",
    "\n",
    "            # Store model state dict (on CPU to save GPU memory)\n",
    "            fold_models_states.append({k: v.cpu() for k, v in trained_fold_model.state_dict().items()})\n",
    "            print(f\"  Fold {fold+1} Test Acc: {fold_test_acc:.4f}\")\n",
    "\n",
    "        # Store results for this configuration\n",
    "        trained_models[config_name] = fold_models_states\n",
    "        # Ensure OOF preds and indices are numpy arrays\n",
    "        oof_predictions[config_name] = np.array(fold_oof_preds)\n",
    "        oof_indices[config_name] = np.array(fold_oof_indices)\n",
    "        cv_scores[config_name] = fold_scores\n",
    "\n",
    "        if fold_scores:\n",
    "             print(f\"  {config_name} Avg CV Accuracy: {np.mean(fold_scores):.4f} +/- {np.std(fold_scores):.4f}\")\n",
    "        else:\n",
    "             print(f\"  {config_name} No successful folds completed.\")\n",
    "\n",
    "\n",
    "    return trained_models, oof_predictions, oof_indices, cv_scores\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# 9. ENSEMBLE PREDICTION & EVALUATION (Unchanged)\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def evaluate_ensemble_cv(oof_predictions, oof_indices, y_true):\n",
    "    \"\"\"Evaluates the ensemble based on out-of-fold predictions.\"\"\"\n",
    "    if not oof_predictions:\n",
    "         print(\"No OOF predictions available to evaluate ensemble.\")\n",
    "         return 0.0, None, None, None # Return None for metrics\n",
    "\n",
    "    all_config_preds_sorted = []\n",
    "    target_indices_sorted = None\n",
    "    valid_configs = list(oof_predictions.keys()) # Start with all configs that have predictions\n",
    "\n",
    "    # Aggregate predictions from all configs, ensuring correct order based on indices\n",
    "    for config_name in list(valid_configs): \n",
    "        preds = oof_predictions[config_name]\n",
    "        indices = oof_indices[config_name]\n",
    "\n",
    "\n",
    "        # Sort predictions based on original sample index\n",
    "        sort_order = np.argsort(indices)\n",
    "        sorted_preds = preds[sort_order]\n",
    "        all_config_preds_sorted.append(sorted_preds)\n",
    "\n",
    "        if target_indices_sorted is None:\n",
    "            target_indices_sorted = indices[sort_order]\n",
    "            if not np.all(target_indices_sorted == np.arange(len(y_true))):\n",
    "                 print(\"Warning: OOF indices do not cover all samples uniquely. Ensemble evaluation might be inaccurate.\")\n",
    "\n",
    "\n",
    "    if not all_config_preds_sorted:\n",
    "         print(\"No valid OOF predictions found after checks. Cannot evaluate ensemble.\")\n",
    "         return 0.0, None, None, None\n",
    "\n",
    "    # Perform majority voting\n",
    "    # Shape: (n_models_in_ensemble, n_samples)\n",
    "    votes = np.array(all_config_preds_sorted)\n",
    "    ensemble_preds = []\n",
    "    for i in range(votes.shape[1]): # Iterate through samples\n",
    "        sample_votes = votes[:, i]\n",
    "        # Find the most frequent prediction (majority vote)\n",
    "        # If there's a tie, np.argmax(np.bincount()) typically returns the smallest index among the tied values.\n",
    "        counts = np.bincount(sample_votes.astype(int))\n",
    "        final_pred = np.argmax(counts)\n",
    "        ensemble_preds.append(final_pred)\n",
    "    ensemble_preds = np.array(ensemble_preds)\n",
    "\n",
    "    # --- Calculate Final Metrics ---\n",
    "    # Ensure y_true corresponds to the sorted order if needed (it should if target_indices_sorted is correct)\n",
    "    accuracy = accuracy_score(y_true, ensemble_preds)\n",
    "    f1_macro = f1_score(y_true, ensemble_preds, average='macro', zero_division=0)\n",
    "    report = classification_report(y_true, ensemble_preds, zero_division=0)\n",
    "    matrix = confusion_matrix(y_true, ensemble_preds)\n",
    "\n",
    "    print(\"\\n--- Final Ensemble Evaluation (Cross-Validated) ---\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(matrix)\n",
    "\n",
    "    return accuracy, f1_macro, report, matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell_10_markdown_main_exec",
   "metadata": {},
   "source": [
    "## 10. Main Execution Block\n",
    "\n",
    "This cell orchestrates the entire workflow:\n",
    "1. Loads the data and creates features using `load_and_preprocess_data`.\n",
    "2. Creates the necessary adjacency matrices using `create_adjacency_matrices`.\n",
    "3. Defines the `main` function which, based on the `RUN_HPO` and `RUN_FINAL_TRAINING` flags:\n",
    "   - Optionally runs hyperparameter optimization (`run_hpo_studies`).\n",
    "   - Optionally trains the top N models found during HPO using cross-validation (`train_final_models_cv`).\n",
    "   - Optionally evaluates the performance of the ensemble model (`evaluate_ensemble_cv`).\n",
    "4. Executes the `main` function within the standard `if __name__ == \"__main__\":` block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell_10_code_main_exec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 0: Loading Data & Preparing Features/Graphs ---\n",
      "Loading data and preparing feature set: 'with_ratios'\n",
      "Calculating ratio features...\n",
      "Added 2 ratio features.\n",
      "Final node feature shape: (40, 64, 7)\n",
      "Data shapes: X_node=(40, 64, 7), y=(40,)\n",
      "Creating adjacency matrices...\n",
      "Adjacency matrices created: ['KNN', 'Threshold', 'AnatomicalRegion', 'CustomBrainNetwork']\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "--- Phase 1: Skipping HPO. Will load parameters from 'hpo_results\\best_hpo_params_all_feat_with_ratios.json' if needed. ---\n",
      "\n",
      "--- Phase 2: Training Final Models with CV and Evaluating Ensemble ---\n",
      "\n",
      "--- Selected Top 5 Configurations for Final Training (Ranked by HPO Score) ---\n",
      "- SimpleGAT_Threshold_feat_with_ratios (HPO Score: 0.8022)\n",
      "- SimpleGAT_AnatomicalRegion_feat_with_ratios (HPO Score: 0.7985)\n",
      "- SpectralGNN_Threshold_feat_with_ratios (HPO Score: 0.7766)\n",
      "- SpectralGNN_AnatomicalRegion_feat_with_ratios (HPO Score: 0.7747)\n",
      "- SimpleGAT_CustomBrainNetwork_feat_with_ratios (HPO Score: 0.7747)\n",
      "Using input channels: 7 (based on FEATURE_SET='with_ratios')\n",
      "\n",
      "--- Training Final Model for: SimpleGAT_Threshold_feat_with_ratios ---\n",
      "  Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Work\\anaconda3\\envs\\toolbox-torch\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 025, Train Loss: 10.8765, Val Loss: 3.8408, Val Acc: 0.5000\n",
      "  Early stopping triggered after epoch 39. Best Val Acc: 0.7500\n",
      "  Fold 1 Test Acc: 0.7500\n",
      "  Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Work\\anaconda3\\envs\\toolbox-torch\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Early stopping triggered after epoch 22. Best Val Acc: 0.7500\n",
      "  Fold 2 Test Acc: 0.7500\n",
      "  Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Work\\anaconda3\\envs\\toolbox-torch\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 025, Train Loss: 0.6968, Val Loss: 9.2631, Val Acc: 0.5000\n",
      "  Early stopping triggered after epoch 38. Best Val Acc: 0.7500\n",
      "  Fold 3 Test Acc: 0.7500\n",
      "  Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Work\\anaconda3\\envs\\toolbox-torch\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Early stopping triggered after epoch 23. Best Val Acc: 0.5000\n",
      "  Fold 4 Test Acc: 0.5000\n",
      "  Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Work\\anaconda3\\envs\\toolbox-torch\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Early stopping triggered after epoch 23. Best Val Acc: 0.6250\n",
      "  Fold 5 Test Acc: 0.6250\n",
      "  SimpleGAT_Threshold_feat_with_ratios Avg CV Accuracy: 0.6750 +/- 0.1000\n",
      "\n",
      "--- Training Final Model for: SimpleGAT_AnatomicalRegion_feat_with_ratios ---\n",
      "  Fold 1/5\n",
      "  Early stopping triggered after epoch 23. Best Val Acc: 0.7500\n",
      "  Fold 1 Test Acc: 0.7500\n",
      "  Fold 2/5\n",
      "  Early stopping triggered after epoch 21. Best Val Acc: 0.5000\n",
      "  Fold 2 Test Acc: 0.5000\n",
      "  Fold 3/5\n",
      "  Epoch: 025, Train Loss: 0.5594, Val Loss: 4.2127, Val Acc: 0.5000\n",
      "  Early stopping triggered after epoch 27. Best Val Acc: 0.7500\n",
      "  Fold 3 Test Acc: 0.7500\n",
      "  Fold 4/5\n",
      "  Epoch: 025, Train Loss: 0.5251, Val Loss: 1.7952, Val Acc: 0.2500\n",
      "  Early stopping triggered after epoch 28. Best Val Acc: 0.6250\n",
      "  Fold 4 Test Acc: 0.6250\n",
      "  Fold 5/5\n",
      "  Early stopping triggered after epoch 21. Best Val Acc: 0.7500\n",
      "  Fold 5 Test Acc: 0.7500\n",
      "  SimpleGAT_AnatomicalRegion_feat_with_ratios Avg CV Accuracy: 0.6750 +/- 0.1000\n",
      "\n",
      "--- Training Final Model for: SpectralGNN_Threshold_feat_with_ratios ---\n",
      "  Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Work\\anaconda3\\envs\\toolbox-torch\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Early stopping triggered after epoch 24. Best Val Acc: 0.3750\n",
      "  Fold 1 Test Acc: 0.3750\n",
      "  Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Work\\anaconda3\\envs\\toolbox-torch\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 025, Train Loss: 0.4120, Val Loss: 0.8215, Val Acc: 0.3750\n",
      "  Early stopping triggered after epoch 27. Best Val Acc: 0.8750\n",
      "  Fold 2 Test Acc: 0.8750\n",
      "  Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Work\\anaconda3\\envs\\toolbox-torch\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Early stopping triggered after epoch 23. Best Val Acc: 0.7500\n",
      "  Fold 3 Test Acc: 0.7500\n",
      "  Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Work\\anaconda3\\envs\\toolbox-torch\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 025, Train Loss: 0.3637, Val Loss: 0.9772, Val Acc: 0.1250\n",
      "  Early stopping triggered after epoch 34. Best Val Acc: 0.5000\n",
      "  Fold 4 Test Acc: 0.5000\n",
      "  Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Work\\anaconda3\\envs\\toolbox-torch\\Lib\\site-packages\\torch_geometric\\warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Early stopping triggered after epoch 21. Best Val Acc: 0.7500\n",
      "  Fold 5 Test Acc: 0.7500\n",
      "  SpectralGNN_Threshold_feat_with_ratios Avg CV Accuracy: 0.6500 +/- 0.1837\n",
      "\n",
      "--- Training Final Model for: SpectralGNN_AnatomicalRegion_feat_with_ratios ---\n",
      "  Fold 1/5\n",
      "  Early stopping triggered after epoch 24. Best Val Acc: 0.6250\n",
      "  Fold 1 Test Acc: 0.6250\n",
      "  Fold 2/5\n",
      "  Epoch: 025, Train Loss: 29.9838, Val Loss: 25.2213, Val Acc: 0.6250\n",
      "  Early stopping triggered after epoch 32. Best Val Acc: 0.8750\n",
      "  Fold 2 Test Acc: 0.8750\n",
      "  Fold 3/5\n",
      "  Epoch: 025, Train Loss: 35.2835, Val Loss: 13.2348, Val Acc: 0.6250\n",
      "  Early stopping triggered after epoch 41. Best Val Acc: 0.7500\n",
      "  Fold 3 Test Acc: 0.7500\n",
      "  Fold 4/5\n",
      "  Early stopping triggered after epoch 21. Best Val Acc: 0.7500\n",
      "  Fold 4 Test Acc: 0.7500\n",
      "  Fold 5/5\n",
      "  Early stopping triggered after epoch 23. Best Val Acc: 0.7500\n",
      "  Fold 5 Test Acc: 0.7500\n",
      "  SpectralGNN_AnatomicalRegion_feat_with_ratios Avg CV Accuracy: 0.7500 +/- 0.0791\n",
      "\n",
      "--- Training Final Model for: SimpleGAT_CustomBrainNetwork_feat_with_ratios ---\n",
      "  Fold 1/5\n",
      "  Epoch: 025, Train Loss: 1.1331, Val Loss: 9.1729, Val Acc: 0.7500\n",
      "  Early stopping triggered after epoch 44. Best Val Acc: 0.8750\n",
      "  Fold 1 Test Acc: 0.8750\n",
      "  Fold 2/5\n",
      "  Early stopping triggered after epoch 22. Best Val Acc: 0.5000\n",
      "  Fold 2 Test Acc: 0.5000\n",
      "  Fold 3/5\n",
      "  Early stopping triggered after epoch 24. Best Val Acc: 0.7500\n",
      "  Fold 3 Test Acc: 0.7500\n",
      "  Fold 4/5\n",
      "  Early stopping triggered after epoch 22. Best Val Acc: 0.6250\n",
      "  Fold 4 Test Acc: 0.6250\n",
      "  Fold 5/5\n",
      "  Epoch: 025, Train Loss: 1.2281, Val Loss: 1.8656, Val Acc: 0.3750\n",
      "  Early stopping triggered after epoch 32. Best Val Acc: 0.7500\n",
      "  Fold 5 Test Acc: 0.7500\n",
      "  SimpleGAT_CustomBrainNetwork_feat_with_ratios Avg CV Accuracy: 0.7000 +/- 0.1275\n",
      "\n",
      "--- Final Ensemble Evaluation (Cross-Validated) ---\n",
      "Accuracy: 0.7500\n",
      "F1 Score (Macro): 0.7442\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.60      0.71        20\n",
      "           1       0.69      0.90      0.78        20\n",
      "\n",
      "    accuracy                           0.75        40\n",
      "   macro avg       0.77      0.75      0.74        40\n",
      "weighted avg       0.77      0.75      0.74        40\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[12  8]\n",
      " [ 2 18]]\n",
      "\n",
      "--- Analysis Complete ---\n"
     ]
    }
   ],
   "source": [
    "#------------------------------\n",
    "# 10. MAIN EXECUTION SCRIPT\n",
    "#------------------------------\n",
    "\n",
    "# --- Phase 0: Load Data & Prepare Features/Graphs ---\n",
    "print(\"--- Phase 0: Loading Data & Preparing Features/Graphs ---\")\n",
    "# Use global FEATURE_SET\n",
    "X_node, y, channel_coords, electrode_mapping_info = load_and_preprocess_data(FEATURE_SET)\n",
    "print(f\"Data shapes: X_node={X_node.shape}, y={y.shape}\")\n",
    "\n",
    "print(\"Creating adjacency matrices...\")\n",
    "adj_matrices = create_adjacency_matrices(channel_coords, electrode_mapping_info)\n",
    "print(f\"Adjacency matrices created: {list(adj_matrices.keys())}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function using pre-loaded data and graphs.\"\"\"\n",
    "\n",
    "    # --- Phase 1: Hyperparameter Optimization ---\n",
    "    # Use global FEATURE_SET\n",
    "    best_hpo_params_path = os.path.join(\"hpo_results\", f'best_hpo_params_all_feat_{FEATURE_SET}.json')\n",
    "    # Use global RUN_HPO flag\n",
    "    if RUN_HPO:\n",
    "        print(\"\\n--- Phase 1: Running Hyperparameter Optimization ---\")\n",
    "        # HPO results (parameters and scores) are saved by run_hpo_studies\n",
    "        run_hpo_studies(X_node, y, adj_matrices, device)\n",
    "    else:\n",
    "        print(f\"\\n--- Phase 1: Skipping HPO. Will load parameters from '{best_hpo_params_path}' if needed. ---\")\n",
    "\n",
    "\n",
    "    # --- Phase 2: Final Model Training & Evaluation ---\n",
    "    # Use global RUN_FINAL_TRAINING flag\n",
    "    if RUN_FINAL_TRAINING:\n",
    "        print(f\"\\n--- Phase 2: Training Final Models with CV and Evaluating Ensemble ---\")\n",
    "        if not os.path.exists(best_hpo_params_path):\n",
    "             print(f\"ERROR: HPO results file '{best_hpo_params_path}' not found. Set RUN_HPO=True to first run HPO\")\n",
    "             return\n",
    "\n",
    "        # Pass required variables explicitly\n",
    "        trained_models, oof_preds, oof_indices, cv_scores = train_final_models_cv(\n",
    "            best_hpo_params_path, X_node, y, adj_matrices, device\n",
    "        )\n",
    "\n",
    "        if trained_models and oof_preds: # Check if training produced results\n",
    "            # Evaluate the ensemble based on OOF predictions\n",
    "            # Pass required variables explicitly\n",
    "            final_accuracy, f1, report, matrix = evaluate_ensemble_cv(oof_preds, oof_indices, y)\n",
    "            print(\"\\n--- Analysis Complete ---\")\n",
    "        else:\n",
    "            print(\"\\n--- Final model training failed or produced no results. Skipping ensemble evaluation. ---\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\n--- Phase 2: Skipping final model training and evaluation. ---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (toolbox-torch)",
   "language": "python",
   "name": "toolbox-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
